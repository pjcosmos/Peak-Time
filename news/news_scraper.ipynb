{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "88c92c18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in /Users/jnpak/.conda/envs/pyexcel/lib/python3.12/site-packages (2.32.5)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/jnpak/.conda/envs/pyexcel/lib/python3.12/site-packages (from requests) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/jnpak/.conda/envs/pyexcel/lib/python3.12/site-packages (from requests) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/jnpak/.conda/envs/pyexcel/lib/python3.12/site-packages (from requests) (2.6.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/jnpak/.conda/envs/pyexcel/lib/python3.12/site-packages (from requests) (2026.1.4)\n"
     ]
    }
   ],
   "source": [
    "# requests ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜ì—¬ë¶€ í™•ì¸\n",
    "!pip install requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d5ba3965",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: beautifulsoup4 in /Users/jnpak/.conda/envs/pyexcel/lib/python3.12/site-packages (4.14.3)\n",
      "Requirement already satisfied: soupsieve>=1.6.1 in /Users/jnpak/.conda/envs/pyexcel/lib/python3.12/site-packages (from beautifulsoup4) (2.8.3)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in /Users/jnpak/.conda/envs/pyexcel/lib/python3.12/site-packages (from beautifulsoup4) (4.15.0)\n"
     ]
    }
   ],
   "source": [
    "# beautifulsoup4 ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜ì—¬ë¶€ í™•ì¸\n",
    "!pip install beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "945140d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reqeusts, bs4 import\n",
    "import requests\n",
    "import bs4\n",
    "# BeautifulSoup í´ë˜ìŠ¤ import\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "65a31856",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2026-02-25 09:42:07] ê¸°í›„ ë°ì´í„° í†µí•© ìˆ˜ì§‘ ì‹œì‘\n",
      "ğŸŒ¡ï¸ ê¸°í›„ ê´€ë ¨ í‚¤ì›Œë“œ 10ê°œë¥¼ ì„¤ì •í–ˆìŠµë‹ˆë‹¤.\n",
      "ğŸ“° í¬í„¸(Daum/Nate) ë‰´ìŠ¤ í†µí•© ìˆ˜ì§‘ ì§„í–‰ ì¤‘...\n",
      " ì§„í–‰ ì¤‘: (10/10) wind warning               \n",
      "âœ¨ ì™„ë£Œ: 10ê°œ í‚¤ì›Œë“œ ë°ì´í„°ë¥¼ climate_news_only.jsonì— ì €ì¥í–ˆìŠµë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import urllib.request\n",
    "import requests\n",
    "from datetime import datetime\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# ìš”ì²­ í—¤ë” (ì‚¬ëŒì²˜ëŸ¼ ë³´ì´ê²Œ ì„¤ì •)\n",
    "REQ_HEADER = {\n",
    "    'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/144.0.0.0 Safari/537.36'\n",
    "}\n",
    "\n",
    "def get_climate_keywords():\n",
    "    \"\"\"ê¸°í›„ ê´€ë ¨ ê³ ì • í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜\"\"\"\n",
    "    keywords = [\n",
    "        \"ë‚ ì”¨\", \"ìš¸ì‚°ë‚ ì”¨\", \"ê°•í’ ê²½ë³´\", \"ë¶€ì‚°ë‚ ì”¨\", \"ë‚ ì”¨ì˜ˆë³´\", \n",
    "        \"ë‚´ì¼ ë‚ ì”¨\", \"ì¼ê¸°ì˜ˆë³´\", \"ëŒ€êµ¬ ë‚ ì”¨\", \"å¤©æ°—\", \"wind warning\"\n",
    "    ]\n",
    "    print(f\"ğŸŒ¡ï¸ ê¸°í›„ ê´€ë ¨ í‚¤ì›Œë“œ {len(keywords)}ê°œë¥¼ ì„¤ì •í–ˆìŠµë‹ˆë‹¤.\")\n",
    "    return keywords\n",
    "\n",
    "def collect_portal_news(portal_name, keyword):\n",
    "    encoded_kw = urllib.parse.quote(keyword)\n",
    "    \n",
    "    if portal_name == \"Daum\":\n",
    "        url = f\"https://search.daum.net/search?w=news&q={encoded_kw}\"\n",
    "    elif portal_name == \"Nate\":\n",
    "        url = f\"https://search.nate.com/search?q={encoded_kw}&thr=ct\"\n",
    "    else:\n",
    "        return []\n",
    "\n",
    "    news_list = []\n",
    "\n",
    "    try:\n",
    "        res = requests.get(url, headers=REQ_HEADER, timeout=5)\n",
    "        if not res.ok:\n",
    "            return news_list\n",
    "\n",
    "        soup = BeautifulSoup(res.text, 'html.parser')\n",
    "\n",
    "        if portal_name == \"Daum\":\n",
    "            a_tags = soup.select(\"div.item-title a\")\n",
    "        else:  # Nate\n",
    "            a_tags = soup.select(\"div.news_wrap a\")\n",
    "\n",
    "        seen_titles = set()\n",
    "\n",
    "        for a_tag in a_tags:\n",
    "            title = a_tag.get_text(strip=True)\n",
    "            link = a_tag.get(\"href\", \"\")\n",
    "\n",
    "            if not link or len(title) < 5:\n",
    "                continue\n",
    "\n",
    "            if link.startswith(\"//\"):\n",
    "                link = \"https:\" + link\n",
    "\n",
    "            if title not in seen_titles:\n",
    "                news_list.append({\"title\": title, \"link\": link})\n",
    "                seen_titles.add(title)\n",
    "\n",
    "            break  # í¬í„¸ë‹¹ 1ê°œ\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ {portal_name} ìˆ˜ì§‘ ì¤‘ ì—ëŸ¬: {e}\")\n",
    "\n",
    "    return news_list\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    now = datetime.now()\n",
    "    print(f\"[{now.strftime('%Y-%m-%d %H:%M:%S')}] ê¸°í›„ ë°ì´í„° í†µí•© ìˆ˜ì§‘ ì‹œì‘\")\n",
    "    \n",
    "    # 1. í‚¤ì›Œë“œ ì„¤ì •\n",
    "    kws = get_climate_keywords()\n",
    "    \n",
    "    if kws:\n",
    "        print(\"ğŸ“° í¬í„¸(Daum/Nate) ë‰´ìŠ¤ í†µí•© ìˆ˜ì§‘ ì§„í–‰ ì¤‘...\")\n",
    "        integrated_data = []\n",
    "        \n",
    "        for i, kw in enumerate(kws):\n",
    "            # ì§„í–‰ ìƒíƒœ ì¶œë ¥\n",
    "            print(f\" ì§„í–‰ ì¤‘: ({i+1}/{len(kws)}) {kw}\" + \" \" * 15, end='\\r')\n",
    "            \n",
    "            # ë‹¤ìŒ ë° ë„¤ì´íŠ¸ ë‰´ìŠ¤ í¬ë¡¤ë§ (ë„¤ì´ë²„ ì œê±°ë¨)\n",
    "            daum_news = collect_portal_news(\"Daum\", kw)\n",
    "            nate_news = collect_portal_news(\"Nate\", kw)\n",
    "            \n",
    "            integrated_data.append({\n",
    "                \"category\": \"ê¸°í›„\",\n",
    "                \"id\": i + 1,\n",
    "                \"keyword\": kw,\n",
    "                \"news\": {\n",
    "                    \"daum\": daum_news,\n",
    "                    \"nate\": nate_news\n",
    "                }\n",
    "            })\n",
    "            # ì°¨ë‹¨ ë°©ì§€ë¥¼ ìœ„í•œ ì•½ê°„ì˜ íœ´ì‹\n",
    "            time.sleep(0.7)\n",
    "\n",
    "        # 2. ìµœì¢… ê²°ê³¼ ì €ì¥\n",
    "        output = {\n",
    "            \"target_keywords\": kws,\n",
    "            \"meta_info\": {\n",
    "                \"source\": \"daum_news & nate_news\",\n",
    "                \"category\": \"climate\",\n",
    "                \"collected_at\": now.isoformat(),\n",
    "                \"count\": len(kws)\n",
    "            },\n",
    "            \"data\": integrated_data\n",
    "        }\n",
    "        \n",
    "        file_name = 'climate_news_only.json'\n",
    "        with open(file_name, 'w', encoding='utf-8') as f:\n",
    "            json.dump(output, f, ensure_ascii=False, indent=4)\n",
    "        \n",
    "        print(f\"\\nâœ¨ ì™„ë£Œ: {len(kws)}ê°œ í‚¤ì›Œë“œ ë°ì´í„°ë¥¼ {file_name}ì— ì €ì¥í–ˆìŠµë‹ˆë‹¤.\")\n",
    "    else:\n",
    "        print(\"í‚¤ì›Œë“œê°€ ì •ì˜ë˜ì§€ ì•Šì•„ ì¤‘ë‹¨í•©ë‹ˆë‹¤.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "55d17ed9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting feedparser\n",
      "  Using cached feedparser-6.0.12-py3-none-any.whl.metadata (2.7 kB)\n",
      "Collecting sgmllib3k (from feedparser)\n",
      "  Using cached sgmllib3k-1.0.0-py3-none-any.whl\n",
      "Using cached feedparser-6.0.12-py3-none-any.whl (81 kB)\n",
      "Installing collected packages: sgmllib3k, feedparser\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2/2\u001b[0m [feedparser]\n",
      "\u001b[1A\u001b[2KSuccessfully installed feedparser-6.0.12 sgmllib3k-1.0.0\n"
     ]
    }
   ],
   "source": [
    "!pip install feedparser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b3108440",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting apscheduler\n",
      "  Downloading apscheduler-3.11.2-py3-none-any.whl.metadata (6.4 kB)\n",
      "Requirement already satisfied: tzlocal>=3.0 in /Users/jnpak/.conda/envs/pyexcel/lib/python3.12/site-packages (from apscheduler) (5.3.1)\n",
      "Downloading apscheduler-3.11.2-py3-none-any.whl (64 kB)\n",
      "Installing collected packages: apscheduler\n",
      "Successfully installed apscheduler-3.11.2\n"
     ]
    }
   ],
   "source": [
    "!pip install apscheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5212352",
   "metadata": {},
   "source": [
    "## Google"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ead1370e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â–¶ Google News ìˆ˜ì§‘: ê°•í’ ê²½ë³´\n",
      "âœ” ì €ì¥ ì™„ë£Œ: ê°•í’ ê²½ë³´ (ê¸°ì‚¬ ìˆ˜: 12ê°œ)\n",
      "â–¶ Google News ìˆ˜ì§‘: ë‚ ì”¨ì˜ˆë³´\n",
      "âœ” ì €ì¥ ì™„ë£Œ: ë‚ ì”¨ì˜ˆë³´ (ê¸°ì‚¬ ìˆ˜: 100ê°œ)\n",
      "â–¶ Google News ìˆ˜ì§‘: ëŒ€êµ¬ ë‚ ì”¨\n",
      "âœ” ì €ì¥ ì™„ë£Œ: ëŒ€êµ¬ ë‚ ì”¨ (ê¸°ì‚¬ ìˆ˜: 40ê°œ)\n",
      "â–¶ Google News ìˆ˜ì§‘: ìš¸ì‚°ë‚ ì”¨\n",
      "âœ” ì €ì¥ ì™„ë£Œ: ìš¸ì‚°ë‚ ì”¨ (ê¸°ì‚¬ ìˆ˜: 35ê°œ)\n",
      "â–¶ Google News ìˆ˜ì§‘: ì¼ê¸°ì˜ˆë³´\n",
      "âœ” ì €ì¥ ì™„ë£Œ: ì¼ê¸°ì˜ˆë³´ (ê¸°ì‚¬ ìˆ˜: 27ê°œ)\n",
      "â–¶ Google News ìˆ˜ì§‘: ì˜¤ëŠ˜ì˜ ë‚ ì”¨\n",
      "âœ” ì €ì¥ ì™„ë£Œ: ì˜¤ëŠ˜ì˜ ë‚ ì”¨ (ê¸°ì‚¬ ìˆ˜: 100ê°œ)\n",
      "\n",
      "âœ… ì™„ë£Œ: ê²°ê³¼ê°€ 'climate_news.json'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import feedparser\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "import urllib.parse\n",
    "\n",
    "# =========================\n",
    "# Notebook ì„¤ì •\n",
    "# =========================\n",
    "KEYWORDS = [\"ê°•í’ ê²½ë³´\", \"ë‚ ì”¨ì˜ˆë³´\", \"ëŒ€êµ¬ ë‚ ì”¨\", \"ìš¸ì‚°ë‚ ì”¨\", \"ì¼ê¸°ì˜ˆë³´\", \"ì˜¤ëŠ˜ì˜ ë‚ ì”¨\"]\n",
    "CATEGORY = \"ê¸°í›„\"\n",
    "COUNTRY = \"ko\"  # í˜„ì¬ëŠ” ko ê³ ì • ì‚¬ìš©\n",
    "JSON_FILE = \"climate_news.json\"\n",
    "\n",
    "REQ_HEADERS = {\n",
    "    \"User-Agent\": (\n",
    "        \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) \"\n",
    "        \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
    "        \"Chrome/120.0.0.0 Safari/537.36\"\n",
    "    )\n",
    "}\n",
    "\n",
    "\n",
    "# =========================\n",
    "# JSON ì €ì¥ í•¨ìˆ˜ (ì§€ì • í¬ë§· + ê¸°ì‚¬ ìˆ˜ í¬í•¨)\n",
    "# =========================\n",
    "def save_news(keyword, article, total_count):\n",
    "    \"\"\"\n",
    "    ë‹¨ì¼ JSON íŒŒì¼(JSON_FILE)ì— í‚¤ì›Œë“œë³„ë¡œ ëˆ„ì  ì €ì¥.\n",
    "    - ê¸°ì¡´ í¬ë§· ìœ ì§€\n",
    "    - total_count: í•´ë‹¹ í‚¤ì›Œë“œë¡œ Google News RSSê°€ ë°˜í™˜í•œ entries ê°œìˆ˜\n",
    "    \"\"\"\n",
    "    if os.path.exists(JSON_FILE):\n",
    "        with open(JSON_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "            content = json.load(f)\n",
    "    else:\n",
    "        content = {\"data\": []}\n",
    "\n",
    "    # ì´ë¯¸ ìˆëŠ” í‚¤ì›Œë“œì¸ì§€ í™•ì¸\n",
    "    for item in content[\"data\"]:\n",
    "        if item[\"keyword\"] == keyword:\n",
    "            # ê¸°ì‚¬ 1ê±´ ì¶”ê°€\n",
    "            item[\"news\"][\"google\"].append(article)\n",
    "            # ìµœì‹  total_count ê°±ì‹ \n",
    "            item[\"total_count\"] = total_count\n",
    "            break\n",
    "    else:\n",
    "        # ì‹ ê·œ í‚¤ì›Œë“œ\n",
    "        content[\"data\"].append({\n",
    "            \"category\": CATEGORY,\n",
    "            \"id\": len(content[\"data\"]) + 1,\n",
    "            \"keyword\": keyword,\n",
    "            \"total_count\": total_count,\n",
    "            \"news\": {\n",
    "                \"google\": [article]\n",
    "            }\n",
    "        })\n",
    "\n",
    "    with open(JSON_FILE, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(content, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Google News ìˆ˜ì§‘ í•¨ìˆ˜ (ìµœì‹  1ê±´ + ì´ ê¸°ì‚¬ ìˆ˜)\n",
    "# =========================\n",
    "def collect_google_news(keyword):\n",
    "    print(f\"â–¶ Google News ìˆ˜ì§‘: {keyword}\")\n",
    "\n",
    "    # í‚¤ì›Œë“œ URL ì¸ì½”ë”© (í•œê¸€/ê³µë°± ì•ˆì „)\n",
    "    encoded_kw = urllib.parse.quote(keyword)\n",
    "    url = f\"https://news.google.com/rss/search?q={encoded_kw}+when:1d&hl=ko&gl=KR&ceid=KR:ko\"\n",
    "\n",
    "    try:\n",
    "        res = requests.get(url, headers=REQ_HEADERS, timeout=10)\n",
    "\n",
    "        if res.status_code != 200:\n",
    "            print(f\"âŒ ìš”ì²­ ì‹¤íŒ¨ (status={res.status_code})\")\n",
    "            # ì‹¤íŒ¨ ì‹œì—ë„ count=0ìœ¼ë¡œ ì €ì¥í•˜ê³  ì‹¶ìœ¼ë©´ ì•„ë˜ ì£¼ì„ í•´ì œ\n",
    "            # save_news(keyword, {\"title\": None, \"link\": None}, 0)\n",
    "            return\n",
    "\n",
    "        feed = feedparser.parse(res.text)\n",
    "        total_count = len(feed.entries)\n",
    "\n",
    "        if total_count == 0:\n",
    "            print(f\"âš ï¸ ê²°ê³¼ ì—†ìŒ (ê¸°ì‚¬ ìˆ˜: {total_count}ê°œ)\")\n",
    "            # ê²°ê³¼ ì—†ìŒë„ ê¸°ë¡í•˜ê³  ì‹¶ìœ¼ë©´ ì•„ë˜ì²˜ëŸ¼ ì €ì¥\n",
    "            save_news(keyword, {\"title\": None, \"link\": None}, total_count)\n",
    "            return\n",
    "\n",
    "        entry = feed.entries[0]\n",
    "\n",
    "        article = {\n",
    "            \"title\": entry.get(\"title\"),\n",
    "            \"link\": entry.get(\"link\")\n",
    "        }\n",
    "\n",
    "        save_news(keyword, article, total_count)\n",
    "        print(f\"âœ” ì €ì¥ ì™„ë£Œ: {keyword} (ê¸°ì‚¬ ìˆ˜: {total_count}ê°œ)\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"âŒ ì˜¤ë¥˜:\", e)\n",
    "        # ì˜¤ë¥˜ë„ ê¸°ë¡í•˜ê³  ì‹¶ìœ¼ë©´ ì•„ë˜ ì£¼ì„ í•´ì œ\n",
    "        # save_news(keyword, {\"title\": None, \"link\": None}, 0)\n",
    "\n",
    "\n",
    "# =========================\n",
    "# ì‹¤í–‰\n",
    "# =========================\n",
    "for kw in KEYWORDS:\n",
    "    collect_google_news(kw)\n",
    "    time.sleep(0.5)\n",
    "\n",
    "print(f\"\\nâœ… ì™„ë£Œ: ê²°ê³¼ê°€ '{JSON_FILE}'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b7f3eba",
   "metadata": {},
   "source": [
    "# Daum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "19fe33c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2026-02-25 11:25:24] Daum ê¸°í›„ ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹œì‘\n",
      "ğŸŒ¡ï¸ ê¸°í›„ ê´€ë ¨ í‚¤ì›Œë“œ 8ê°œë¥¼ ì„¤ì •í–ˆìŠµë‹ˆë‹¤.\n",
      "ğŸ“° ë‹¤ìŒ(Daum) ë‰´ìŠ¤ ìˆ˜ì§‘ ì§„í–‰ ì¤‘...\n",
      " ì§„í–‰ ì¤‘: (8/8) ëŒ€êµ¬ ë‚ ì”¨               \n",
      "âœ¨ ì™„ë£Œ: 8ê°œ í‚¤ì›Œë“œ ë°ì´í„°ë¥¼ climate_daum_news_only.jsonì— ì €ì¥í–ˆìŠµë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import requests\n",
    "import urllib.parse\n",
    "from datetime import datetime\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# ìš”ì²­ í—¤ë” (ì‚¬ëŒì²˜ëŸ¼ ë³´ì´ê²Œ ì„¤ì •)\n",
    "REQ_HEADER = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/144.0.0.0 Safari/537.36\"\n",
    "}\n",
    "\n",
    "def get_climate_keywords():\n",
    "    \"\"\"ê¸°í›„ ê´€ë ¨ ê³ ì • í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜\"\"\"\n",
    "    keywords = [\n",
    "        \"ë‚ ì”¨\", \"ìš¸ì‚°ë‚ ì”¨\", \"ê°•í’ ê²½ë³´\", \"ë¶€ì‚°ë‚ ì”¨\", \"ë‚ ì”¨ì˜ˆë³´\",\n",
    "        \"ë‚´ì¼ ë‚ ì”¨\", \"ì¼ê¸°ì˜ˆë³´\", \"ëŒ€êµ¬ ë‚ ì”¨\"\n",
    "    ]\n",
    "    print(f\"ğŸŒ¡ï¸ ê¸°í›„ ê´€ë ¨ í‚¤ì›Œë“œ {len(keywords)}ê°œë¥¼ ì„¤ì •í–ˆìŠµë‹ˆë‹¤.\")\n",
    "    return keywords\n",
    "\n",
    "def collect_daum_news(keyword, max_items=1):\n",
    "    \"\"\"\n",
    "    Daum ë‰´ìŠ¤ ê²€ìƒ‰ì—ì„œ ê¸°ì‚¬ ì¶”ì¶œ\n",
    "    - news_list: ê¸°ì‚¬ ë¦¬ìŠ¤íŠ¸ (ìµœëŒ€ max_itemsê°œ)\n",
    "    - total_count: í˜„ì¬ ê²€ìƒ‰ ê²°ê³¼ í˜ì´ì§€ì—ì„œ íŒŒì‹±ëœ 'ê¸°ì‚¬ ë§í¬ í›„ë³´' ê°œìˆ˜\n",
    "    \"\"\"\n",
    "    encoded_kw = urllib.parse.quote(keyword)\n",
    "    url = f\"https://search.daum.net/search?w=news&q={encoded_kw}\"\n",
    "\n",
    "    news_list = []\n",
    "    total_count = 0\n",
    "\n",
    "    try:\n",
    "        res = requests.get(url, headers=REQ_HEADER, timeout=5)\n",
    "        if not res.ok:\n",
    "            return news_list, total_count\n",
    "\n",
    "        soup = BeautifulSoup(res.text, \"html.parser\")\n",
    "\n",
    "        # ë‹¤ìŒ ë‰´ìŠ¤ ê²€ìƒ‰ ê²°ê³¼: ë³´í†µ div.item-title a í˜•íƒœ\n",
    "        a_tags = soup.select(\"div.item-title a\")\n",
    "\n",
    "        seen = set()\n",
    "        candidates = []\n",
    "\n",
    "        for a in a_tags:\n",
    "            title = a.get_text(strip=True)\n",
    "            link = a.get(\"href\", \"\")\n",
    "\n",
    "            # ê¸°ë³¸ í•„í„°\n",
    "            if not link or link == \"#\" or len(title) < 5:\n",
    "                continue\n",
    "\n",
    "            if link.startswith(\"//\"):\n",
    "                link = \"https:\" + link\n",
    "\n",
    "            # ì¤‘ë³µ ì œê±° (title ê¸°ì¤€)\n",
    "            if title in seen:\n",
    "                continue\n",
    "\n",
    "            seen.add(title)\n",
    "            candidates.append({\"title\": title, \"link\": link})\n",
    "\n",
    "        # âœ… ê¸°ì‚¬ ìˆ˜(=í˜„ì¬ í˜ì´ì§€ì—ì„œ íŒŒì‹±ëœ í›„ë³´ ê¸°ì‚¬ ìˆ˜)\n",
    "        total_count = len(candidates)\n",
    "\n",
    "        # âœ… ì‹¤ì œ ì €ì¥í•  ê¸°ì‚¬ëŠ” max_itemsê°œë§Œ\n",
    "        news_list = candidates[:max_items]\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Daum ìˆ˜ì§‘ ì¤‘ ì—ëŸ¬: {e}\")\n",
    "\n",
    "    return news_list, total_count\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    now = datetime.now()\n",
    "    print(f\"[{now.strftime('%Y-%m-%d %H:%M:%S')}] Daum ê¸°í›„ ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹œì‘\")\n",
    "\n",
    "    # 1) í‚¤ì›Œë“œ ì„¤ì •\n",
    "    kws = get_climate_keywords()\n",
    "\n",
    "    if not kws:\n",
    "        print(\"í‚¤ì›Œë“œê°€ ì •ì˜ë˜ì§€ ì•Šì•„ ì¤‘ë‹¨í•©ë‹ˆë‹¤.\")\n",
    "        raise SystemExit\n",
    "\n",
    "    print(\"ğŸ“° ë‹¤ìŒ(Daum) ë‰´ìŠ¤ ìˆ˜ì§‘ ì§„í–‰ ì¤‘...\")\n",
    "    integrated_data = []\n",
    "\n",
    "    for i, kw in enumerate(kws):\n",
    "        print(f\" ì§„í–‰ ì¤‘: ({i+1}/{len(kws)}) {kw}\" + \" \" * 15, end=\"\\r\")\n",
    "\n",
    "        daum_news, total_count = collect_daum_news(kw, max_items=1)\n",
    "\n",
    "        integrated_data.append({\n",
    "            \"category\": \"ê¸°í›„\",\n",
    "            \"id\": i + 1,\n",
    "            \"keyword\": kw,\n",
    "            \"total_count\": total_count,   # âœ… ê¸°ì‚¬ ìˆ˜ ì €ì¥\n",
    "            \"news\": {\n",
    "                \"daum\": daum_news\n",
    "            }\n",
    "        })\n",
    "\n",
    "        time.sleep(0.7)\n",
    "\n",
    "    # 2) ìµœì¢… ê²°ê³¼ ì €ì¥\n",
    "    output = {\n",
    "        \"target_keywords\": kws,\n",
    "        \"meta_info\": {\n",
    "            \"source\": \"daum_news\",\n",
    "            \"category\": \"climate\",\n",
    "            \"collected_at\": now.isoformat(),\n",
    "            \"count\": len(kws)\n",
    "        },\n",
    "        \"data\": integrated_data\n",
    "    }\n",
    "\n",
    "    file_name = \"climate_daum_news_only.json\"\n",
    "    with open(file_name, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(output, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "    print(f\"\\nâœ¨ ì™„ë£Œ: {len(kws)}ê°œ í‚¤ì›Œë“œ ë°ì´í„°ë¥¼ {file_name}ì— ì €ì¥í–ˆìŠµë‹ˆë‹¤.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91e39f3d",
   "metadata": {},
   "source": [
    "# Naver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c47ab8ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… JSON ì €ì¥ ì™„ë£Œ: naver_news_climate.json\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import urllib.parse\n",
    "import time\n",
    "import re\n",
    "import json\n",
    "from datetime import datetime, timezone, timedelta\n",
    "\n",
    "# =========================\n",
    "# ì„¤ì •\n",
    "# =========================\n",
    "KEYWORDS = [\"ë‚ ì”¨\", \"ìš¸ì‚°ë‚ ì”¨\", \"ê°•í’ ê²½ë³´\", \"ë¶€ì‚°ë‚ ì”¨\", \"ë‚ ì”¨ì˜ˆë³´\", \"ë‚´ì¼ ë‚ ì”¨\", \"ì¼ê¸°ì˜ˆë³´\", \"ëŒ€êµ¬ ë‚ ì”¨\"]\n",
    "\n",
    "HEADERS = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/144.0.0.0 Safari/537.36\",\n",
    "    \"Accept-Language\": \"ko-KR,ko;q=0.9,en;q=0.8\",\n",
    "    \"Referer\": \"https://www.naver.com/\",\n",
    "}\n",
    "\n",
    "KST = timezone(timedelta(hours=9))\n",
    "\n",
    "\n",
    "# =========================\n",
    "# ìœ í‹¸: ë°œí–‰ì¼ë¡œ ë³´ì´ëŠ” í…ìŠ¤íŠ¸ íŒë³„\n",
    "# =========================\n",
    "def looks_like_time_or_date(txt: str) -> bool:\n",
    "    if not txt:\n",
    "        return False\n",
    "    t = txt.strip()\n",
    "    if re.search(r\"\\d+\\s*(ë¶„|ì‹œê°„|ì¼|ì£¼|ê°œì›”|ë…„)\\s*ì „\", t):\n",
    "        return True\n",
    "    if re.search(r\"\\d{4}\\.\\d{2}\\.\\d{2}\\.?\", t):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "\n",
    "# =========================\n",
    "# ë°œí–‰ì¼ ì •ê·œí™”: 'YYYY-MM-DD HH:MM' (KST)\n",
    "# =========================\n",
    "def normalize_published(published_raw: str):\n",
    "    if not published_raw:\n",
    "        return None\n",
    "\n",
    "    s = published_raw.strip()\n",
    "\n",
    "    # 1) ISO8601 (ì˜ˆ: 2026-02-25T01:07:20+09:00)\n",
    "    try:\n",
    "        dt = datetime.fromisoformat(s.replace(\"Z\", \"+00:00\"))\n",
    "        if dt.tzinfo is None:\n",
    "            dt = dt.replace(tzinfo=KST)\n",
    "        dt = dt.astimezone(KST)\n",
    "        return dt.strftime(\"%Y-%m-%d %H:%M\")\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # 2) ë‚ ì§œí˜• (ì˜ˆ: 2026.02.25 or 2026.02.25.)\n",
    "    m = re.search(r\"(\\d{4})\\.(\\d{2})\\.(\\d{2})\", s)\n",
    "    if m:\n",
    "        y, mo, d = map(int, m.groups())\n",
    "        dt = datetime(y, mo, d, 0, 0, tzinfo=KST)\n",
    "        return dt.strftime(\"%Y-%m-%d %H:%M\")\n",
    "\n",
    "    # 3) ìƒëŒ€ì‹œê°„ (ì˜ˆ: 3ì‹œê°„ ì „)\n",
    "    m = re.search(r\"(\\d+)\\s*(ë¶„|ì‹œê°„|ì¼|ì£¼|ê°œì›”|ë…„)\\s*ì „\", s)\n",
    "    if m:\n",
    "        n = int(m.group(1))\n",
    "        unit = m.group(2)\n",
    "        now = datetime.now(KST)\n",
    "\n",
    "        if unit == \"ë¶„\":\n",
    "            dt = now - timedelta(minutes=n)\n",
    "        elif unit == \"ì‹œê°„\":\n",
    "            dt = now - timedelta(hours=n)\n",
    "        elif unit == \"ì¼\":\n",
    "            dt = now - timedelta(days=n)\n",
    "        elif unit == \"ì£¼\":\n",
    "            dt = now - timedelta(weeks=n)\n",
    "        elif unit == \"ê°œì›”\":\n",
    "            dt = now - timedelta(days=30 * n)   # ê·¼ì‚¬\n",
    "        elif unit == \"ë…„\":\n",
    "            dt = now - timedelta(days=365 * n)  # ê·¼ì‚¬\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "        return dt.strftime(\"%Y-%m-%d %H:%M\")\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "# =========================\n",
    "# ê¸°ì‚¬ í˜ì´ì§€ì—ì„œ ë°œí–‰ì¼ ì¶”ì¶œ ë³´ì¡°: JSON-LD\n",
    "# =========================\n",
    "def parse_published_from_jsonld(soup: BeautifulSoup):\n",
    "    for script in soup.select('script[type=\"application/ld+json\"]'):\n",
    "        try:\n",
    "            txt = script.get_text(strip=True)\n",
    "            if not txt:\n",
    "                continue\n",
    "            data = json.loads(txt)\n",
    "            candidates = data if isinstance(data, list) else [data]\n",
    "            for obj in candidates:\n",
    "                if isinstance(obj, dict):\n",
    "                    dp = obj.get(\"datePublished\") or obj.get(\"dateCreated\") or obj.get(\"dateModified\")\n",
    "                    if dp:\n",
    "                        return dp\n",
    "        except Exception:\n",
    "            continue\n",
    "    return None\n",
    "\n",
    "\n",
    "# =========================\n",
    "# ê¸°ì‚¬ í˜ì´ì§€ì—ì„œ ë°œí–‰ì¼ ì¶”ì¶œ ë³´ì¡°: meta\n",
    "# =========================\n",
    "def parse_published_from_meta(soup: BeautifulSoup):\n",
    "    meta_props = [\n",
    "        (\"property\", \"article:published_time\"),\n",
    "        (\"property\", \"og:article:published_time\"),\n",
    "        (\"name\", \"article:published_time\"),\n",
    "        (\"name\", \"pubdate\"),\n",
    "        (\"name\", \"date\"),\n",
    "    ]\n",
    "    for attr, key in meta_props:\n",
    "        tag = soup.find(\"meta\", attrs={attr: key})\n",
    "        if tag and tag.get(\"content\"):\n",
    "            return tag[\"content\"].strip()\n",
    "    return None\n",
    "\n",
    "\n",
    "# =========================\n",
    "# ì›ë¬¸ì—ì„œ ë°œí–‰ì¼ ì¶”ì¶œ\n",
    "# =========================\n",
    "def fetch_published_from_article(link: str):\n",
    "    try:\n",
    "        res = requests.get(link, headers=HEADERS, timeout=10)\n",
    "        res.raise_for_status()\n",
    "        soup = BeautifulSoup(res.text, \"html.parser\")\n",
    "\n",
    "        pub = parse_published_from_meta(soup)\n",
    "        if pub:\n",
    "            return pub\n",
    "\n",
    "        pub = parse_published_from_jsonld(soup)\n",
    "        if pub:\n",
    "            return pub\n",
    "\n",
    "        # ìµœí›„: í…ìŠ¤íŠ¸ì—ì„œ ë‚ ì§œ íŒ¨í„´\n",
    "        text = soup.get_text(\" \", strip=True)\n",
    "        m = re.search(r\"\\d{4}\\.\\d{2}\\.\\d{2}\\.?\\s*\\d{1,2}:\\d{2}\", text)\n",
    "        if m:\n",
    "            return m.group(0)\n",
    "\n",
    "        m = re.search(r\"\\d{4}\\.\\d{2}\\.\\d{2}\\.?\", text)\n",
    "        if m:\n",
    "            return m.group(0)\n",
    "\n",
    "        return None\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "\n",
    "# =========================\n",
    "# ë„¤ì´ë²„ ë‰´ìŠ¤ ê²€ìƒ‰: í‚¤ì›Œë“œë³„ 1ê°œ ì¶”ì¶œ\n",
    "# =========================\n",
    "def fetch_one_naver_news(keyword: str):\n",
    "    q = urllib.parse.quote(keyword)\n",
    "    url = f\"https://search.naver.com/search.naver?where=news&sm=tab_jum&query={q}\"\n",
    "\n",
    "    res = requests.get(url, headers=HEADERS, timeout=10)\n",
    "    res.raise_for_status()\n",
    "    soup = BeautifulSoup(res.text, \"html.parser\")\n",
    "\n",
    "    # ì œëª©/ë§í¬ (ì‘ë™í•˜ë˜ ë°©ì‹ ìš°ì„ )\n",
    "    a = soup.select_one('a[data-heatmap-target=\".tit\"]')\n",
    "    if not a:\n",
    "        a = soup.select_one(\"a.news_tit\")\n",
    "    if not a:\n",
    "        return None\n",
    "\n",
    "    title = a.get_text(strip=True)\n",
    "    link = a.get(\"href\")\n",
    "\n",
    "    # 1ì°¨: ê²€ìƒ‰ ê²°ê³¼ ì¹´ë“œì—ì„œ ë°œí–‰ì¼ ì‹œë„\n",
    "    published = None\n",
    "    card = a.find_parent(\"div\", class_=\"news_area\")\n",
    "    if not card:\n",
    "        card = a.find_parent([\"div\", \"li\"])\n",
    "\n",
    "    if card:\n",
    "        infos = card.select(\"div.info_group span.info, span.info\")\n",
    "        for sp in infos:\n",
    "            txt = sp.get_text(strip=True)\n",
    "            if looks_like_time_or_date(txt):\n",
    "                published = txt\n",
    "                break\n",
    "\n",
    "    # 2ì°¨: ì›ë¬¸ì—ì„œ ë°œí–‰ì¼ ë³´ê°•\n",
    "    if published is None and link:\n",
    "        published = fetch_published_from_article(link)\n",
    "\n",
    "    return {\n",
    "        \"keyword\": keyword,\n",
    "        \"title\": title,\n",
    "        \"published_raw\": published,\n",
    "        \"published\": normalize_published(published),\n",
    "        \"link\": link\n",
    "    }\n",
    "\n",
    "\n",
    "# =========================\n",
    "# ì‹¤í–‰\n",
    "# =========================\n",
    "# rowsì—ì„œ published_raw í‚¤ ì œê±° (JSONì— ì•ˆ ë‚˜ì˜¤ê²Œ)\n",
    "rows_no_raw = []\n",
    "for item in rows:\n",
    "    if isinstance(item, dict):\n",
    "        new_item = dict(item)\n",
    "        new_item.pop(\"published_raw\", None)\n",
    "        rows_no_raw.append(new_item)\n",
    "    else:\n",
    "        rows_no_raw.append(item)\n",
    "        \n",
    "rows = []\n",
    "for kw in KEYWORDS:\n",
    "    try:\n",
    "        item = fetch_one_naver_news(kw)\n",
    "        if item is None:\n",
    "            rows.append({\"keyword\": kw, \"title\": None, \"published\": None, \"published_raw\": None, \"link\": None})\n",
    "        else:\n",
    "            rows.append(item)\n",
    "    except Exception as e:\n",
    "        rows.append({\"keyword\": kw, \"title\": None, \"published\": None, \"published_raw\": None, \"link\": None, \"error\": str(e)})\n",
    "\n",
    "    time.sleep(0.5)  # ì›ë¬¸ê¹Œì§€ ë“¤ì–´ê°€ë¯€ë¡œ ë”œë ˆì´ ê¶Œì¥\n",
    "\n",
    "df = pd.DataFrame(rows, columns=[\"keyword\", \"title\", \"published\", \"link\"])\n",
    "df\n",
    "\n",
    "# =========================\n",
    "# JSON ì €ì¥ (ì¶”ê°€)\n",
    "# =========================\n",
    "OUT_FILE = \"naver_news_climate.json\"\n",
    "\n",
    "output = {\n",
    "    \"target_keywords\": KEYWORDS,\n",
    "    \"meta_info\": {\n",
    "        \"source\": \"naver_news_search\",\n",
    "        \"category\": \"climate\",\n",
    "        \"collected_at\": datetime.now().isoformat(),\n",
    "        \"count\": len(KEYWORDS),\n",
    "        \"note\": \"1 article per keyword (title/link from Naver search; published normalized when possible)\"\n",
    "    },\n",
    "    \"data\": rows_no_raw\n",
    "}\n",
    "\n",
    "with open(OUT_FILE, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(output, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(f\"âœ… JSON ì €ì¥ ì™„ë£Œ: {OUT_FILE}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyexcel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
