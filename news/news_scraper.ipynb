{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "88c92c18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in /Users/jnpak/.conda/envs/pyexcel/lib/python3.12/site-packages (2.32.5)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/jnpak/.conda/envs/pyexcel/lib/python3.12/site-packages (from requests) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/jnpak/.conda/envs/pyexcel/lib/python3.12/site-packages (from requests) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/jnpak/.conda/envs/pyexcel/lib/python3.12/site-packages (from requests) (2.6.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/jnpak/.conda/envs/pyexcel/lib/python3.12/site-packages (from requests) (2026.1.4)\n"
     ]
    }
   ],
   "source": [
    "# requests 라이브러리 설치여부 확인\n",
    "!pip install requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d5ba3965",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: beautifulsoup4 in /Users/jnpak/.conda/envs/pyexcel/lib/python3.12/site-packages (4.14.3)\n",
      "Requirement already satisfied: soupsieve>=1.6.1 in /Users/jnpak/.conda/envs/pyexcel/lib/python3.12/site-packages (from beautifulsoup4) (2.8.3)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in /Users/jnpak/.conda/envs/pyexcel/lib/python3.12/site-packages (from beautifulsoup4) (4.15.0)\n"
     ]
    }
   ],
   "source": [
    "# beautifulsoup4 라이브러리 설치여부 확인\n",
    "!pip install beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "945140d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reqeusts, bs4 import\n",
    "import requests\n",
    "import bs4\n",
    "# BeautifulSoup 클래스 import\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "55d17ed9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting feedparser\n",
      "  Using cached feedparser-6.0.12-py3-none-any.whl.metadata (2.7 kB)\n",
      "Collecting sgmllib3k (from feedparser)\n",
      "  Using cached sgmllib3k-1.0.0-py3-none-any.whl\n",
      "Using cached feedparser-6.0.12-py3-none-any.whl (81 kB)\n",
      "Installing collected packages: sgmllib3k, feedparser\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/2\u001b[0m [feedparser]\n",
      "\u001b[1A\u001b[2KSuccessfully installed feedparser-6.0.12 sgmllib3k-1.0.0\n"
     ]
    }
   ],
   "source": [
    "!pip install feedparser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b3108440",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting apscheduler\n",
      "  Downloading apscheduler-3.11.2-py3-none-any.whl.metadata (6.4 kB)\n",
      "Requirement already satisfied: tzlocal>=3.0 in /Users/jnpak/.conda/envs/pyexcel/lib/python3.12/site-packages (from apscheduler) (5.3.1)\n",
      "Downloading apscheduler-3.11.2-py3-none-any.whl (64 kB)\n",
      "Installing collected packages: apscheduler\n",
      "Successfully installed apscheduler-3.11.2\n"
     ]
    }
   ],
   "source": [
    "!pip install apscheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5212352",
   "metadata": {},
   "source": [
    "## Google"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "cffe6b30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 진행 중: [GoogleRSS] [스포츠] (10/10) 맨 시티 대 뉴캐슬                                    \n",
      "✅ 완료: google_news_grouped_by_category_keyword.json 저장\n",
      "   - 구조: 카테고리 > 키워드 > { total_count: {google}, articles: [기사row] }\n",
      "   - published_at/collected_at: YYYY-MM-DD HH:MM\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import feedparser\n",
    "import json\n",
    "import time\n",
    "import urllib.parse\n",
    "import re\n",
    "from datetime import datetime, timezone, timedelta\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# =========================\n",
    "# 1) 입력: 카테고리/키워드\n",
    "# =========================\n",
    "CATEGORIES = {\n",
    "    \"기후\": [\"날씨\", \"울산날씨\", \"강풍 경보\", \"부산날씨\", \"날씨예보\", \"내일 날씨\", \"일기예보\", \"대구 날씨\"],\n",
    "    \"엔터테인먼트\": [\"유튜브 문제가 발생했습니다\", \"문상민\", \"파반느\", \"부산찬란한 너의 계절에\", \"김태리\", \"아너 그녀들의 법정\", \"윤영경\", \"정동원\", \"금잔디\", \"홍자\"],\n",
    "    \"비즈니스 및 금융\": [\"김인호\", \"비트코인\", \"케이뱅크 공모주\", \"하이닉스 주가\", \"엔화\", \"주식\", \"에어로케이\", \"에스팀\", \"신영자\", \"액스비스\"],\n",
    "    \"스포츠\": [\"엘에이 fc 대 인터 마이애미\", \"mls\", \"챔피언스리그\", \"인테르\", \"토트넘 대 아스널\", \"노시환\", \"레알 에스파냐 대 엘에이 fc\", \"psg 대 메스\", \"울브스 대 아스널\", \"맨 시티 대 뉴캐슬\"]\n",
    "}\n",
    "\n",
    "COUNTRY = \"KR\"\n",
    "LANG = \"ko\"\n",
    "OUT_FILE = \"google_news_grouped_by_category_keyword.json\"\n",
    "\n",
    "REQ_HEADERS = {\n",
    "    \"User-Agent\": (\n",
    "        \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) \"\n",
    "        \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
    "        \"Chrome/120.0.0.0 Safari/537.36\"\n",
    "    ),\n",
    "    \"Accept-Language\": \"ko-KR,ko;q=0.9,en;q=0.8\",\n",
    "    \"Referer\": \"https://news.google.com/\",\n",
    "}\n",
    "\n",
    "KST = timezone(timedelta(hours=9))\n",
    "\n",
    "# =========================\n",
    "# 2) 시간 전처리 (YYYY-MM-DD HH:MM)\n",
    "# =========================\n",
    "def format_hhmm(dt: datetime) -> str:\n",
    "    if dt.tzinfo is None:\n",
    "        dt = dt.replace(tzinfo=KST)\n",
    "    dt = dt.astimezone(KST)\n",
    "    return dt.strftime(\"%Y-%m-%d %H:%M\")\n",
    "\n",
    "def normalize_published_to_hhmm(raw: str) -> str:\n",
    "    if not raw:\n",
    "        return \"\"\n",
    "    s = raw.strip()\n",
    "\n",
    "    # ISO8601\n",
    "    if \"T\" in s and \"-\" in s:\n",
    "        try:\n",
    "            dt = datetime.fromisoformat(s.replace(\"Z\", \"+00:00\"))\n",
    "            if dt.tzinfo is None:\n",
    "                dt = dt.replace(tzinfo=KST)\n",
    "            return format_hhmm(dt.astimezone(KST))\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    # RFC822\n",
    "    try:\n",
    "        s2 = re.sub(r\"\\bGMT\\b\", \"+0000\", s)\n",
    "        dt = datetime.strptime(s2, \"%a, %d %b %Y %H:%M:%S %z\")\n",
    "        return format_hhmm(dt.astimezone(KST))\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    return \"\"\n",
    "\n",
    "# =========================\n",
    "# 3) 원문 링크에서 publisher / og:image 보강(선택)\n",
    "# =========================\n",
    "def abs_url(url: str) -> str:\n",
    "    if not url:\n",
    "        return \"\"\n",
    "    u = url.strip()\n",
    "    if u.startswith(\"//\"):\n",
    "        return \"https:\" + u\n",
    "    return u\n",
    "\n",
    "def fetch_publisher_and_image(url: str, timeout=8):\n",
    "    out = {\"publisher\": \"\", \"image_url\": \"\"}\n",
    "    if not url:\n",
    "        return out\n",
    "\n",
    "    try:\n",
    "        res = requests.get(url, headers=REQ_HEADERS, timeout=timeout, allow_redirects=True)\n",
    "        if not res.ok:\n",
    "            return out\n",
    "\n",
    "        soup = BeautifulSoup(res.text, \"html.parser\")\n",
    "\n",
    "        og_img = soup.select_one('meta[property=\"og:image\"]')\n",
    "        if og_img and og_img.get(\"content\"):\n",
    "            out[\"image_url\"] = abs_url(og_img[\"content\"].strip())\n",
    "\n",
    "        og_site = soup.select_one('meta[property=\"og:site_name\"]')\n",
    "        if og_site and og_site.get(\"content\"):\n",
    "            out[\"publisher\"] = og_site[\"content\"].strip()\n",
    "\n",
    "        # json-ld publisher.name 보강\n",
    "        if not out[\"publisher\"]:\n",
    "            for script in soup.select('script[type=\"application/ld+json\"]'):\n",
    "                try:\n",
    "                    txt = script.get_text(strip=True)\n",
    "                    if not txt:\n",
    "                        continue\n",
    "                    data = json.loads(txt)\n",
    "                    candidates = data if isinstance(data, list) else [data]\n",
    "                    for obj in candidates:\n",
    "                        if isinstance(obj, dict):\n",
    "                            pub = obj.get(\"publisher\")\n",
    "                            if isinstance(pub, dict):\n",
    "                                name = pub.get(\"name\")\n",
    "                                if isinstance(name, str) and name.strip():\n",
    "                                    out[\"publisher\"] = name.strip()\n",
    "                                    break\n",
    "                    if out[\"publisher\"]:\n",
    "                        break\n",
    "                except Exception:\n",
    "                    continue\n",
    "\n",
    "        return out\n",
    "    except Exception:\n",
    "        return out\n",
    "\n",
    "# =========================\n",
    "# 4) Google RSS url + 폴백\n",
    "# =========================\n",
    "def build_google_rss_url(query: str, days: int | None):\n",
    "    q = query if days is None else f\"{query} when:{days}d\"\n",
    "    encoded_q = urllib.parse.quote(q)\n",
    "    return f\"https://news.google.com/rss/search?q={encoded_q}&hl={LANG}&gl={COUNTRY}&ceid={COUNTRY}:{LANG}\"\n",
    "\n",
    "def collect_google_top1_and_count(keyword: str):\n",
    "    \"\"\"\n",
    "    반환:\n",
    "      - news: {title,url,publisher,published_at,image_url}\n",
    "      - total_count: int  (RSS가 반환한 entries 개수)\n",
    "    폴백:\n",
    "      1) \"키워드\" when:1d\n",
    "      2) 키워드 when:1d\n",
    "      3) \"키워드\" when:7d\n",
    "      4) 키워드 when:7d\n",
    "      5) \"키워드\" 기간제한 없음\n",
    "      6) 키워드 기간제한 없음\n",
    "    \"\"\"\n",
    "    empty_news = {\"title\": \"\", \"url\": \"\", \"publisher\": \"\", \"published_at\": \"\", \"image_url\": \"\"}\n",
    "    variants = [\n",
    "        (f'\"{keyword}\"', 1),\n",
    "        (f\"{keyword}\", 1),\n",
    "        (f'\"{keyword}\"', 7),\n",
    "        (f\"{keyword}\", 7),\n",
    "        (f'\"{keyword}\"', None),\n",
    "        (f\"{keyword}\", None),\n",
    "    ]\n",
    "\n",
    "    best_count = 0\n",
    "\n",
    "    for q, days in variants:\n",
    "        rss_url = build_google_rss_url(q, days)\n",
    "        try:\n",
    "            res = requests.get(rss_url, headers=REQ_HEADERS, timeout=10)\n",
    "            if res.status_code != 200:\n",
    "                continue\n",
    "\n",
    "            feed = feedparser.parse(res.text)\n",
    "            entries = feed.entries or []\n",
    "            best_count = max(best_count, len(entries))\n",
    "\n",
    "            if not entries:\n",
    "                continue\n",
    "\n",
    "            e = entries[0]\n",
    "            title = (e.get(\"title\") or \"\").strip()\n",
    "            link = (e.get(\"link\") or \"\").strip()\n",
    "\n",
    "            published_raw = (e.get(\"published\") or e.get(\"updated\") or \"\").strip()\n",
    "            published_at = normalize_published_to_hhmm(published_raw)\n",
    "\n",
    "            publisher = \"\"\n",
    "            src = e.get(\"source\")\n",
    "            if isinstance(src, dict) and src.get(\"title\"):\n",
    "                publisher = str(src[\"title\"]).strip()\n",
    "            elif isinstance(src, str):\n",
    "                publisher = src.strip()\n",
    "\n",
    "            meta = fetch_publisher_and_image(link)\n",
    "            if not publisher:\n",
    "                publisher = (meta.get(\"publisher\") or \"\").strip()\n",
    "            image_url = (meta.get(\"image_url\") or \"\").strip()\n",
    "\n",
    "            news = {\n",
    "                \"title\": title or \"\",\n",
    "                \"url\": link or \"\",\n",
    "                \"publisher\": publisher or \"\",\n",
    "                \"published_at\": published_at or \"\",\n",
    "                \"image_url\": image_url or \"\"\n",
    "            }\n",
    "            return news, len(entries)\n",
    "\n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "    # 전부 실패했으면 (뉴스는 empty, total_count는 best_count(대부분 0))\n",
    "    return empty_news, best_count\n",
    "\n",
    "# =========================\n",
    "# 5) 메인: 카테고리 > 키워드 > { total_count, articles }\n",
    "#    - articles 내부 row는 DB 컬럼 9키만 유지\n",
    "# =========================\n",
    "if __name__ == \"__main__\":\n",
    "    collected_at = format_hhmm(datetime.now(KST))\n",
    "\n",
    "    # run_id(임시): 카테고리별 1개\n",
    "    run_id_by_category = {cat: idx + 1 for idx, cat in enumerate(CATEGORIES.keys())}\n",
    "\n",
    "    # keyword_id(임시): 전체 유니크\n",
    "    keyword_id_by_text = {}\n",
    "    kid = 1\n",
    "    for cat, kws in CATEGORIES.items():\n",
    "        for kw in kws:\n",
    "            if kw not in keyword_id_by_text:\n",
    "                keyword_id_by_text[kw] = kid\n",
    "                kid += 1\n",
    "\n",
    "    grouped = {}\n",
    "    article_id = 1\n",
    "\n",
    "    for cat, kws in CATEGORIES.items():\n",
    "        grouped.setdefault(cat, {})\n",
    "        run_id = run_id_by_category[cat]\n",
    "\n",
    "        for i, kw in enumerate(kws, start=1):\n",
    "            print(f\" 진행 중: [GoogleRSS] [{cat}] ({i}/{len(kws)}) {kw}\" + \" \" * 30, end=\"\\r\")\n",
    "\n",
    "            news, total_count = collect_google_top1_and_count(kw)\n",
    "\n",
    "            row = {\n",
    "                \"article_id\": article_id,\n",
    "                \"run_id\": run_id,\n",
    "                \"keyword_id\": keyword_id_by_text[kw],\n",
    "                \"title\": news.get(\"title\") or None,\n",
    "                \"url\": news.get(\"url\") or None,\n",
    "                \"publisher\": news.get(\"publisher\") or None,\n",
    "                \"published_at\": news.get(\"published_at\") or None,\n",
    "                \"image_url\": news.get(\"image_url\") or None,\n",
    "                \"collected_at\": collected_at\n",
    "            }\n",
    "\n",
    "            # ✅ 기사 row는 DB 컬럼 키만 유지\n",
    "            row = {k: row[k] for k in [\n",
    "                \"article_id\", \"run_id\", \"keyword_id\",\n",
    "                \"title\", \"url\", \"publisher\", \"published_at\",\n",
    "                \"image_url\", \"collected_at\"\n",
    "            ]}\n",
    "\n",
    "            # ✅ 키워드 레벨에 total_count.google 저장 (기사 row에는 넣지 않음)\n",
    "            grouped[cat][kw] = {\n",
    "                \"total_count\": {\"google\": int(total_count)},\n",
    "                \"articles\": [row]\n",
    "            }\n",
    "\n",
    "            article_id += 1\n",
    "            time.sleep(0.5)\n",
    "\n",
    "    with open(OUT_FILE, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(grouped, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "    print(f\"\\n✅ 완료: {OUT_FILE} 저장\")\n",
    "    print(\"   - 구조: 카테고리 > 키워드 > { total_count: {google}, articles: [기사row] }\")\n",
    "    print(\"   - published_at/collected_at: YYYY-MM-DD HH:MM\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b7f3eba",
   "metadata": {},
   "source": [
    "# Daum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "d2e9cd87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 진행 중: [스포츠] (10/10) 맨 시티 대 뉴캐슬                                    \n",
      "✨ 완료: daum_news_grouped_by_category_keyword.json 저장 (카테고리>키워드>기사row), collected_at/published_at=YYYY-MM-DD HH:MM\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import time\n",
    "import requests\n",
    "import urllib.parse\n",
    "import re\n",
    "from datetime import datetime, timezone, timedelta\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# =========================\n",
    "# 1) 입력: 카테고리/키워드\n",
    "# =========================\n",
    "CATEGORIES = {\n",
    "    \"기후\": [\"날씨\", \"울산날씨\", \"강풍 경보\", \"부산날씨\", \"날씨예보\", \"내일 날씨\", \"일기예보\", \"대구 날씨\"],\n",
    "    \"엔터테인먼트\": [\"유튜브 문제가 발생했습니다\", \"문상민\", \"파반느\", \"부산찬란한 너의 계절에\", \"김태리\", \"아너 그녀들의 법정\", \"윤영경\", \"정동원\", \"금잔디\", \"홍자\"],\n",
    "    \"비즈니스 및 금융\": [\"김인호\", \"비트코인\", \"케이뱅크 공모주\", \"하이닉스 주가\", \"엔화\", \"주식\", \"에어로케이\", \"에스팀\", \"신영자\", \"액스비스\"],\n",
    "    \"스포츠\": [\"엘에이 fc 대 인터 마이애미\", \"mls\", \"챔피언스리그\", \"인테르\", \"토트넘 대 아스널\", \"노시환\", \"레알 에스파냐 대 엘에이 fc\", \"psg 대 메스\", \"울브스 대 아스널\", \"맨 시티 대 뉴캐슬\"]\n",
    "}\n",
    "\n",
    "REQ_HEADER = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/144.0.0.0 Safari/537.36\",\n",
    "    \"Accept-Language\": \"ko-KR,ko;q=0.9,en;q=0.8\",\n",
    "    \"Referer\": \"https://www.daum.net/\",\n",
    "}\n",
    "\n",
    "KST = timezone(timedelta(hours=9))\n",
    "\n",
    "# =========================\n",
    "# 2) 유틸\n",
    "# =========================\n",
    "def now_kst():\n",
    "    return datetime.now(tz=KST)\n",
    "\n",
    "def format_hhmm(dt: datetime) -> str:\n",
    "    if dt.tzinfo is None:\n",
    "        dt = dt.replace(tzinfo=KST)\n",
    "    dt = dt.astimezone(KST)\n",
    "    return dt.strftime(\"%Y-%m-%d %H:%M\")\n",
    "\n",
    "def abs_url(url: str) -> str:\n",
    "    if not url:\n",
    "        return \"\"\n",
    "    url = url.strip()\n",
    "    if url.startswith(\"//\"):\n",
    "        return \"https:\" + url\n",
    "    return url\n",
    "\n",
    "def first_attr(soup, selectors, attr):\n",
    "    if not soup:\n",
    "        return \"\"\n",
    "    for sel in selectors:\n",
    "        el = soup.select_one(sel)\n",
    "        if el and el.get(attr):\n",
    "            v = el.get(attr, \"\").strip()\n",
    "            if v:\n",
    "                return v\n",
    "    return \"\"\n",
    "\n",
    "def first_text(soup, selectors):\n",
    "    if not soup:\n",
    "        return \"\"\n",
    "    for sel in selectors:\n",
    "        el = soup.select_one(sel)\n",
    "        if el:\n",
    "            t = el.get_text(\" \", strip=True)\n",
    "            if t:\n",
    "                return t\n",
    "    return \"\"\n",
    "\n",
    "def parse_ldjson(soup: BeautifulSoup):\n",
    "    \"\"\"\n",
    "    ld+json에서 publisher/name, datePublished 등을 최대한 추출\n",
    "    \"\"\"\n",
    "    publisher = \"\"\n",
    "    date_published = \"\"\n",
    "    try:\n",
    "        for script in soup.select('script[type=\"application/ld+json\"]'):\n",
    "            txt = script.get_text(strip=True)\n",
    "            if not txt:\n",
    "                continue\n",
    "            data = json.loads(txt)\n",
    "\n",
    "            def walk(obj):\n",
    "                nonlocal publisher, date_published\n",
    "                if isinstance(obj, dict):\n",
    "                    for k in (\"datePublished\", \"dateCreated\", \"dateModified\"):\n",
    "                        v = obj.get(k)\n",
    "                        if not date_published and isinstance(v, str) and v.strip():\n",
    "                            date_published = v.strip()\n",
    "\n",
    "                    pub = obj.get(\"publisher\")\n",
    "                    if not publisher:\n",
    "                        if isinstance(pub, dict):\n",
    "                            name = pub.get(\"name\")\n",
    "                            if isinstance(name, str) and name.strip():\n",
    "                                publisher = name.strip()\n",
    "                        elif isinstance(pub, list):\n",
    "                            for it in pub:\n",
    "                                if isinstance(it, dict):\n",
    "                                    name = it.get(\"name\")\n",
    "                                    if isinstance(name, str) and name.strip():\n",
    "                                        publisher = name.strip()\n",
    "                                        break\n",
    "\n",
    "                    for v in obj.values():\n",
    "                        walk(v)\n",
    "\n",
    "                elif isinstance(obj, list):\n",
    "                    for it in obj:\n",
    "                        walk(it)\n",
    "\n",
    "            walk(data)\n",
    "\n",
    "            if publisher or date_published:\n",
    "                break\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    return publisher, date_published\n",
    "\n",
    "# =========================\n",
    "# 3) 시간 전처리: published_at 을 YYYY-MM-DD HH:MM 로\n",
    "# =========================\n",
    "def to_dt_from_yyyymmddhhmmss(raw: str):\n",
    "    digits = \"\".join(ch for ch in (raw or \"\") if ch.isdigit())\n",
    "    if len(digits) < 12:\n",
    "        return None\n",
    "    y = int(digits[0:4])\n",
    "    mo = int(digits[4:6])\n",
    "    d = int(digits[6:8])\n",
    "    hh = int(digits[8:10])\n",
    "    mm = int(digits[10:12])\n",
    "    ss = int(digits[12:14]) if len(digits) >= 14 else 0\n",
    "    return datetime(y, mo, d, hh, mm, ss, tzinfo=KST)\n",
    "\n",
    "def normalize_published_to_hhmm(raw: str) -> str:\n",
    "    \"\"\"\n",
    "    가능한 경우 'YYYY-MM-DD HH:MM' (KST)로 변환\n",
    "    - og:regDate(YYYYMMDDHHMMSS) 처리\n",
    "    - ISO 문자열 처리\n",
    "    \"\"\"\n",
    "    if not raw:\n",
    "        return \"\"\n",
    "\n",
    "    s = raw.strip()\n",
    "    digits = \"\".join(ch for ch in s if ch.isdigit())\n",
    "\n",
    "    # 1) 14자리/12자리 기반\n",
    "    if len(digits) >= 12:\n",
    "        dt = to_dt_from_yyyymmddhhmmss(digits)\n",
    "        return format_hhmm(dt) if dt else \"\"\n",
    "\n",
    "    # 2) ISO8601\n",
    "    if \"T\" in s and \"-\" in s:\n",
    "        try:\n",
    "            s2 = s.replace(\"Z\", \"+00:00\")\n",
    "            dt = datetime.fromisoformat(s2)\n",
    "            if dt.tzinfo is None:\n",
    "                dt = dt.replace(tzinfo=KST)\n",
    "            dt = dt.astimezone(KST)\n",
    "            return format_hhmm(dt)\n",
    "        except Exception:\n",
    "            return \"\"\n",
    "\n",
    "    return \"\"\n",
    "\n",
    "# =========================\n",
    "# 4) 기사(또는 v.daum.net 뷰어)에서 메타 추출\n",
    "# =========================\n",
    "def fetch_article_meta(url: str, timeout=8):\n",
    "    \"\"\"\n",
    "    기사(또는 v.daum.net 뷰어) 페이지에서:\n",
    "    - publisher\n",
    "    - published_at (YYYY-MM-DD HH:MM)\n",
    "    - image_url\n",
    "    추출\n",
    "    \"\"\"\n",
    "    meta = {\"publisher\": \"\", \"published_at\": \"\", \"image_url\": \"\"}\n",
    "\n",
    "    if not url:\n",
    "        return meta\n",
    "\n",
    "    try:\n",
    "        res = requests.get(url, headers=REQ_HEADER, timeout=timeout, allow_redirects=True)\n",
    "        if not res.ok:\n",
    "            return meta\n",
    "\n",
    "        soup = BeautifulSoup(res.text, \"html.parser\")\n",
    "\n",
    "        # image_url: og:image 우선\n",
    "        og_image = first_attr(soup, ['meta[property=\"og:image\"]', 'meta[name=\"og:image\"]'], \"content\")\n",
    "        meta[\"image_url\"] = abs_url(og_image)\n",
    "\n",
    "        # published_at 우선순위:\n",
    "        # 1) og:regDate (Daum 뷰어에서 매우 자주 14자리)\n",
    "        regdate = first_attr(soup, ['meta[property=\"og:regDate\"]'], \"content\")\n",
    "        if regdate:\n",
    "            meta[\"published_at\"] = normalize_published_to_hhmm(regdate)\n",
    "\n",
    "        # 2) article:published_time\n",
    "        if not meta[\"published_at\"]:\n",
    "            apub = first_attr(\n",
    "                soup,\n",
    "                [\n",
    "                    'meta[property=\"article:published_time\"]',\n",
    "                    'meta[name=\"article:published_time\"]',\n",
    "                    'meta[property=\"og:article:published_time\"]',\n",
    "                    'meta[name=\"pubdate\"]',\n",
    "                    'meta[name=\"publishdate\"]',\n",
    "                    'meta[name=\"date\"]',\n",
    "                ],\n",
    "                \"content\",\n",
    "            )\n",
    "            meta[\"published_at\"] = normalize_published_to_hhmm(apub)\n",
    "\n",
    "        # 3) ld+json datePublished\n",
    "        ld_publisher, ld_date = parse_ldjson(soup)\n",
    "        if not meta[\"published_at\"] and ld_date:\n",
    "            meta[\"published_at\"] = normalize_published_to_hhmm(ld_date)\n",
    "\n",
    "        # publisher 우선순위:\n",
    "        # 1) ld+json publisher.name\n",
    "        meta[\"publisher\"] = ld_publisher.strip() if ld_publisher else \"\"\n",
    "\n",
    "        # 2) og:site_name\n",
    "        if not meta[\"publisher\"]:\n",
    "            site_name = first_attr(soup, ['meta[property=\"og:site_name\"]', 'meta[name=\"og:site_name\"]'], \"content\")\n",
    "            meta[\"publisher\"] = site_name.strip() if site_name else \"\"\n",
    "\n",
    "        # 3) Daum 뷰어 DOM fallback\n",
    "        if (not meta[\"publisher\"]) and (\"v.daum.net\" in url):\n",
    "            meta[\"publisher\"] = first_text(\n",
    "                soup,\n",
    "                [\n",
    "                    \"em.info_cp a\",\n",
    "                    \"span.info_cp a\",\n",
    "                    \"a.link_cp\",\n",
    "                    \"span.txt_cp\",\n",
    "                    \"em.txt_cp\",\n",
    "                ],\n",
    "            )\n",
    "\n",
    "        return meta\n",
    "\n",
    "    except Exception:\n",
    "        return meta\n",
    "\n",
    "# =========================\n",
    "# 5) Daum 검색에서 top1 기사 추출\n",
    "# =========================\n",
    "def collect_daum_top1(keyword: str):\n",
    "    \"\"\"\n",
    "    다음 검색에서 키워드 1개당 1개 기사:\n",
    "    title/url 뽑고,\n",
    "    원문 열어서 publisher/published_at/image_url 보강\n",
    "    \"\"\"\n",
    "    encoded_kw = urllib.parse.quote(keyword)\n",
    "    search_url = f\"https://search.daum.net/search?w=news&q={encoded_kw}\"\n",
    "\n",
    "    empty = {\"title\": \"\", \"url\": \"\", \"publisher\": \"\", \"published_at\": \"\", \"image_url\": \"\"}\n",
    "\n",
    "    try:\n",
    "        res = requests.get(search_url, headers=REQ_HEADER, timeout=8)\n",
    "        if not res.ok:\n",
    "            return empty\n",
    "\n",
    "        soup = BeautifulSoup(res.text, \"html.parser\")\n",
    "\n",
    "        # 제목 링크 후보\n",
    "        a_tags = soup.select(\"div.item-title a\")\n",
    "        if not a_tags:\n",
    "            a_tags = soup.select(\"a.f_link_b, a.link_tit\")\n",
    "\n",
    "        for a in a_tags:\n",
    "            title = a.get_text(strip=True)\n",
    "            url = abs_url(a.get(\"href\", \"\"))\n",
    "\n",
    "            if not url or url == \"#\" or len(title) < 5:\n",
    "                continue\n",
    "\n",
    "            # 원문 메타 보강(여기서 image_url/ publisher / published_at)\n",
    "            meta = fetch_article_meta(url)\n",
    "\n",
    "            return {\n",
    "                \"title\": title,\n",
    "                \"url\": url,\n",
    "                \"publisher\": meta.get(\"publisher\", \"\") or \"\",\n",
    "                \"published_at\": meta.get(\"published_at\", \"\") or \"\",  # ✅ YYYY-MM-DD HH:MM\n",
    "                \"image_url\": meta.get(\"image_url\", \"\") or \"\"\n",
    "            }\n",
    "\n",
    "        return empty\n",
    "\n",
    "    except Exception:\n",
    "        return empty\n",
    "\n",
    "# =========================\n",
    "# 6) 메인: (카테고리 > 키워드 > 기사row) + 시간 전처리 동일\n",
    "#     기사 row는 DB 컬럼 키만 유지\n",
    "# =========================\n",
    "if __name__ == \"__main__\":\n",
    "    collected_at = format_hhmm(now_kst())  # ✅ YYYY-MM-DD HH:MM\n",
    "\n",
    "    # 임시 ID 매핑 (DB INSERT 전 단계에서 FK 연결용)\n",
    "    # run_id: 카테고리별 1개 run 생성(1~4)\n",
    "    run_id_by_category = {}\n",
    "    rid = 1\n",
    "    for cat in CATEGORIES.keys():\n",
    "        run_id_by_category[cat] = rid\n",
    "        rid += 1\n",
    "\n",
    "    # keyword_id: 전체 키워드 유니크하게 1..N\n",
    "    keyword_id_by_text = {}\n",
    "    kid = 1\n",
    "    for cat, kws in CATEGORIES.items():\n",
    "        for kw in kws:\n",
    "            if kw not in keyword_id_by_text:\n",
    "                keyword_id_by_text[kw] = kid\n",
    "                kid += 1\n",
    "\n",
    "    grouped = {}\n",
    "    article_id = 1\n",
    "\n",
    "    for cat, kws in CATEGORIES.items():\n",
    "        grouped.setdefault(cat, {})\n",
    "        run_id = run_id_by_category[cat]\n",
    "\n",
    "        for i, kw in enumerate(kws, start=1):\n",
    "            print(f\" 진행 중: [{cat}] ({i}/{len(kws)}) {kw}\" + \" \" * 30, end=\"\\r\")\n",
    "\n",
    "            news = collect_daum_top1(kw)\n",
    "\n",
    "            row = {\n",
    "                \"article_id\": article_id,\n",
    "                \"run_id\": run_id,\n",
    "                \"keyword_id\": keyword_id_by_text[kw],\n",
    "                \"title\": news.get(\"title\") or None,\n",
    "                \"url\": news.get(\"url\") or None,\n",
    "                \"publisher\": news.get(\"publisher\") or None,\n",
    "                \"published_at\": news.get(\"published_at\") or None,   # ✅ YYYY-MM-DD HH:MM\n",
    "                \"image_url\": news.get(\"image_url\") or None,\n",
    "                \"collected_at\": collected_at                         # ✅ YYYY-MM-DD HH:MM\n",
    "            }\n",
    "\n",
    "            # ✅ row는 DB키만 유지(안전장치)\n",
    "            row = {k: row[k] for k in [\n",
    "                \"article_id\", \"run_id\", \"keyword_id\",\n",
    "                \"title\", \"url\", \"publisher\", \"published_at\",\n",
    "                \"image_url\", \"collected_at\"\n",
    "            ]}\n",
    "\n",
    "            grouped[cat].setdefault(kw, []).append(row)\n",
    "\n",
    "            article_id += 1\n",
    "            time.sleep(0.7)\n",
    "\n",
    "    out_file = \"daum_news_grouped_by_category_keyword.json\"\n",
    "    with open(out_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(grouped, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "    print(f\"\\n✨ 완료: {out_file} 저장 (카테고리>{'키워드'}>기사row), collected_at/published_at=YYYY-MM-DD HH:MM\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91e39f3d",
   "metadata": {},
   "source": [
    "# Naver"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a96833c2",
   "metadata": {},
   "source": [
    "모든 카테고리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "18de5a8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 진행 중: [스포츠] (10/10) 맨 시티 대 뉴캐슬                          \n",
      "✅ JSON 저장 완료: naver_news_grouped_by_category_keyword.json\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib.parse\n",
    "import time\n",
    "import re\n",
    "import json\n",
    "from datetime import datetime, timezone, timedelta\n",
    "\n",
    "# =========================\n",
    "# 설정\n",
    "# =========================\n",
    "CATEGORIES = {\n",
    "    \"기후\": [\"날씨\", \"울산날씨\", \"강풍 경보\", \"부산날씨\", \"날씨예보\", \"내일 날씨\", \"일기예보\", \"대구 날씨\"],\n",
    "    \"엔터테인먼트\": [\"유튜브 문제가 발생했습니다\", \"문상민\", \"파반느\", \"부산찬란한 너의 계절에\", \"김태리\", \"아너 그녀들의 법정\", \"윤영경\", \"정동원\", \"금잔디\", \"홍자\"],\n",
    "    \"비즈니스 및 금융\": [\"김인호\", \"비트코인\", \"케이뱅크 공모주\", \"하이닉스 주가\", \"엔화\", \"주식\", \"에어로케이\", \"에스팀\", \"신영자\", \"액스비스\"],\n",
    "    \"스포츠\": [\"엘에이 fc 대 인터 마이애미\", \"mls\", \"챔피언스리그\", \"인테르\", \"토트넘 대 아스널\", \"노시환\", \"레알 에스파냐 대 엘에이 fc\", \"psg 대 메스\", \"울브스 대 아스널\", \"맨 시티 대 뉴캐슬\"]\n",
    "}\n",
    "\n",
    "HEADERS = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/144.0.0.0 Safari/537.36\",\n",
    "    \"Accept-Language\": \"ko-KR,ko;q=0.9,en;q=0.8\",\n",
    "    \"Referer\": \"https://www.naver.com/\",\n",
    "}\n",
    "\n",
    "KST = timezone(timedelta(hours=9))\n",
    "\n",
    "# =========================\n",
    "# collected_at 전처리 (HH:MM까지만)\n",
    "# =========================\n",
    "def format_collected_at(dt: datetime) -> str:\n",
    "    if dt.tzinfo is None:\n",
    "        dt = dt.replace(tzinfo=KST)\n",
    "    dt = dt.astimezone(KST)\n",
    "    return dt.strftime(\"%Y-%m-%d %H:%M\")\n",
    "\n",
    "# =========================\n",
    "# 유틸: 발행일로 보이는 텍스트 판별\n",
    "# =========================\n",
    "def looks_like_time_or_date(txt: str) -> bool:\n",
    "    if not txt:\n",
    "        return False\n",
    "    t = txt.strip()\n",
    "    if re.search(r\"\\d+\\s*(분|시간|일|주|개월|년)\\s*전\", t):\n",
    "        return True\n",
    "    if re.search(r\"\\d{4}\\.\\d{2}\\.\\d{2}\\.?\", t):\n",
    "        return True\n",
    "    if re.search(r\"\\d{4}-\\d{2}-\\d{2}\", t):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "# =========================\n",
    "# 발행일 정규화: 'YYYY-MM-DD HH:MM' (KST)  ✅ 그대로 유지\n",
    "# =========================\n",
    "def normalize_published(published_raw: str):\n",
    "    if not published_raw:\n",
    "        return None\n",
    "\n",
    "    s = published_raw.strip()\n",
    "\n",
    "    # 1) ISO8601\n",
    "    try:\n",
    "        dt = datetime.fromisoformat(s.replace(\"Z\", \"+00:00\"))\n",
    "        if dt.tzinfo is None:\n",
    "            dt = dt.replace(tzinfo=KST)\n",
    "        dt = dt.astimezone(KST)\n",
    "        return dt.strftime(\"%Y-%m-%d %H:%M\")\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # 2) 날짜+시간 (2026.02.25. 13:10)\n",
    "    m = re.search(r\"(\\d{4})\\.(\\d{2})\\.(\\d{2})\\.?\\s*(\\d{1,2}):(\\d{2})\", s)\n",
    "    if m:\n",
    "        y, mo, d, hh, mm = m.groups()\n",
    "        dt = datetime(int(y), int(mo), int(d), int(hh), int(mm), tzinfo=KST)\n",
    "        return dt.strftime(\"%Y-%m-%d %H:%M\")\n",
    "\n",
    "    # 3) 날짜만 (2026.02.25)\n",
    "    m = re.search(r\"(\\d{4})\\.(\\d{2})\\.(\\d{2})\", s)\n",
    "    if m:\n",
    "        y, mo, d = map(int, m.groups())\n",
    "        dt = datetime(y, mo, d, 0, 0, tzinfo=KST)\n",
    "        return dt.strftime(\"%Y-%m-%d %H:%M\")\n",
    "\n",
    "    # 4) 상대시간 (3시간 전)\n",
    "    m = re.search(r\"(\\d+)\\s*(분|시간|일|주|개월|년)\\s*전\", s)\n",
    "    if m:\n",
    "        n = int(m.group(1))\n",
    "        unit = m.group(2)\n",
    "        now = datetime.now(KST)\n",
    "\n",
    "        if unit == \"분\":\n",
    "            dt = now - timedelta(minutes=n)\n",
    "        elif unit == \"시간\":\n",
    "            dt = now - timedelta(hours=n)\n",
    "        elif unit == \"일\":\n",
    "            dt = now - timedelta(days=n)\n",
    "        elif unit == \"주\":\n",
    "            dt = now - timedelta(weeks=n)\n",
    "        elif unit == \"개월\":\n",
    "            dt = now - timedelta(days=30 * n)\n",
    "        elif unit == \"년\":\n",
    "            dt = now - timedelta(days=365 * n)\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "        return dt.strftime(\"%Y-%m-%d %H:%M\")\n",
    "\n",
    "    # 5) 이미 YYYY-MM-DD HH:MM 이면 그대로\n",
    "    if re.fullmatch(r\"\\d{4}-\\d{2}-\\d{2}\\s+\\d{2}:\\d{2}\", s):\n",
    "        return s\n",
    "\n",
    "    return None\n",
    "\n",
    "# =========================\n",
    "# 기사 페이지에서 발행일 추출 보조\n",
    "# =========================\n",
    "def parse_published_from_jsonld(soup: BeautifulSoup):\n",
    "    for script in soup.select('script[type=\"application/ld+json\"]'):\n",
    "        try:\n",
    "            txt = script.get_text(strip=True)\n",
    "            if not txt:\n",
    "                continue\n",
    "            data = json.loads(txt)\n",
    "            candidates = data if isinstance(data, list) else [data]\n",
    "            for obj in candidates:\n",
    "                if isinstance(obj, dict):\n",
    "                    dp = obj.get(\"datePublished\") or obj.get(\"dateCreated\") or obj.get(\"dateModified\")\n",
    "                    if dp:\n",
    "                        return dp\n",
    "        except Exception:\n",
    "            continue\n",
    "    return None\n",
    "\n",
    "def parse_published_from_meta(soup: BeautifulSoup):\n",
    "    meta_props = [\n",
    "        (\"property\", \"article:published_time\"),\n",
    "        (\"property\", \"og:article:published_time\"),\n",
    "        (\"name\", \"article:published_time\"),\n",
    "        (\"name\", \"pubdate\"),\n",
    "        (\"name\", \"date\"),\n",
    "    ]\n",
    "    for attr, key in meta_props:\n",
    "        tag = soup.find(\"meta\", attrs={attr: key})\n",
    "        if tag and tag.get(\"content\"):\n",
    "            return tag[\"content\"].strip()\n",
    "    return None\n",
    "\n",
    "def fetch_published_raw_from_article(link: str):\n",
    "    try:\n",
    "        res = requests.get(link, headers=HEADERS, timeout=10)\n",
    "        res.raise_for_status()\n",
    "        soup = BeautifulSoup(res.text, \"html.parser\")\n",
    "\n",
    "        pub = parse_published_from_meta(soup) or parse_published_from_jsonld(soup)\n",
    "        if pub:\n",
    "            return pub\n",
    "\n",
    "        text = soup.get_text(\" \", strip=True)\n",
    "        m = re.search(r\"\\d{4}\\.\\d{2}\\.\\d{2}\\.?\\s*\\d{1,2}:\\d{2}\", text)\n",
    "        if m:\n",
    "            return m.group(0)\n",
    "\n",
    "        m = re.search(r\"\\d{4}\\.\\d{2}\\.\\d{2}\\.?\", text)\n",
    "        if m:\n",
    "            return m.group(0)\n",
    "\n",
    "        return None\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "# =========================\n",
    "# 언론사 추출\n",
    "# =========================\n",
    "def parse_publisher_from_jsonld(soup: BeautifulSoup):\n",
    "    for script in soup.select('script[type=\"application/ld+json\"]'):\n",
    "        try:\n",
    "            txt = script.get_text(strip=True)\n",
    "            if not txt:\n",
    "                continue\n",
    "            data = json.loads(txt)\n",
    "            candidates = data if isinstance(data, list) else [data]\n",
    "            for obj in candidates:\n",
    "                if isinstance(obj, dict):\n",
    "                    pub = obj.get(\"publisher\")\n",
    "                    if isinstance(pub, dict):\n",
    "                        name = pub.get(\"name\")\n",
    "                        if isinstance(name, str) and name.strip():\n",
    "                            return name.strip()\n",
    "                    if isinstance(pub, list):\n",
    "                        for it in pub:\n",
    "                            if isinstance(it, dict):\n",
    "                                name = it.get(\"name\")\n",
    "                                if isinstance(name, str) and name.strip():\n",
    "                                    return name.strip()\n",
    "        except Exception:\n",
    "            continue\n",
    "    return None\n",
    "\n",
    "def fetch_publisher_from_article(link: str):\n",
    "    try:\n",
    "        res = requests.get(link, headers=HEADERS, timeout=10)\n",
    "        res.raise_for_status()\n",
    "        soup = BeautifulSoup(res.text, \"html.parser\")\n",
    "\n",
    "        pub = parse_publisher_from_jsonld(soup)\n",
    "        if pub:\n",
    "            return pub\n",
    "\n",
    "        tag = soup.find(\"meta\", attrs={\"property\": \"og:site_name\"})\n",
    "        if tag and tag.get(\"content\"):\n",
    "            return tag[\"content\"].strip()\n",
    "\n",
    "        return None\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "# =========================\n",
    "# 이미지 추출\n",
    "# =========================\n",
    "def fetch_og_image(link: str):\n",
    "    try:\n",
    "        res = requests.get(link, headers=HEADERS, timeout=10)\n",
    "        res.raise_for_status()\n",
    "        soup = BeautifulSoup(res.text, \"html.parser\")\n",
    "\n",
    "        tag = soup.find(\"meta\", attrs={\"property\": \"og:image\"})\n",
    "        if tag and tag.get(\"content\"):\n",
    "            return tag[\"content\"].strip()\n",
    "\n",
    "        tag = soup.find(\"meta\", attrs={\"name\": \"twitter:image\"})\n",
    "        if tag and tag.get(\"content\"):\n",
    "            return tag[\"content\"].strip()\n",
    "\n",
    "        return None\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "# =========================\n",
    "# 네이버 뉴스 검색: 키워드별 1개\n",
    "# =========================\n",
    "def fetch_one_naver_news(keyword: str):\n",
    "    q = urllib.parse.quote(keyword)\n",
    "    url = f\"https://search.naver.com/search.naver?where=news&sm=tab_jum&query={q}\"\n",
    "\n",
    "    res = requests.get(url, headers=HEADERS, timeout=10)\n",
    "    res.raise_for_status()\n",
    "    soup = BeautifulSoup(res.text, \"html.parser\")\n",
    "\n",
    "    a = soup.select_one('a[data-heatmap-target=\".tit\"]') or soup.select_one(\"a.news_tit\")\n",
    "    if not a:\n",
    "        return None\n",
    "\n",
    "    title = a.get_text(strip=True)\n",
    "    link = a.get(\"href\")\n",
    "\n",
    "    published_raw = None\n",
    "    publisher = None\n",
    "    card = a.find_parent(\"div\", class_=\"news_area\") or a.find_parent([\"div\", \"li\"])\n",
    "\n",
    "    if card:\n",
    "        infos = card.select(\"div.info_group span.info, span.info\")\n",
    "        for sp in infos:\n",
    "            txt = sp.get_text(strip=True)\n",
    "            if looks_like_time_or_date(txt):\n",
    "                published_raw = txt\n",
    "                break\n",
    "\n",
    "        press = card.select_one(\"a.info.press, span.info.press, a.press, span.press\")\n",
    "        if press:\n",
    "            publisher = press.get_text(\" \", strip=True)\n",
    "\n",
    "    if published_raw is None and link:\n",
    "        published_raw = fetch_published_raw_from_article(link)\n",
    "\n",
    "    published = normalize_published(published_raw)\n",
    "\n",
    "    if (not publisher) and link:\n",
    "        publisher = fetch_publisher_from_article(link)\n",
    "\n",
    "    img_url = None\n",
    "    if card:\n",
    "        img = card.select_one(\"img\")\n",
    "        if img:\n",
    "            img_url = (\n",
    "                img.get(\"data-lazy-src\")\n",
    "                or img.get(\"data-src\")\n",
    "                or img.get(\"src\")\n",
    "                or img.get(\"data-original\")\n",
    "            )\n",
    "            if img_url and img_url.startswith(\"//\"):\n",
    "                img_url = \"https:\" + img_url\n",
    "\n",
    "    if (not img_url) and link:\n",
    "        img_url = fetch_og_image(link)\n",
    "\n",
    "    return {\n",
    "        \"title\": title,\n",
    "        \"url\": link,\n",
    "        \"publisher\": publisher,\n",
    "        \"published_at\": published,     # ✅ 'YYYY-MM-DD HH:MM' 유지\n",
    "        \"image_url\": img_url\n",
    "    }\n",
    "\n",
    "# =========================\n",
    "# 실행: 카테고리 > 키워드 > [news_article row]\n",
    "# =========================\n",
    "if __name__ == \"__main__\":\n",
    "    collected_at = format_collected_at(datetime.now(KST))  # ✅ HH:MM까지만\n",
    "\n",
    "    # run_id: 카테고리별 1개 run (임시)\n",
    "    run_id_by_category = {}\n",
    "    rid = 1\n",
    "    for cat in CATEGORIES.keys():\n",
    "        run_id_by_category[cat] = rid\n",
    "        rid += 1\n",
    "\n",
    "    # keyword_id: 전체 키워드 유니크 ID (임시)\n",
    "    keyword_id_by_text = {}\n",
    "    kid = 1\n",
    "    for cat, kws in CATEGORIES.items():\n",
    "        for kw in kws:\n",
    "            if kw not in keyword_id_by_text:\n",
    "                keyword_id_by_text[kw] = kid\n",
    "                kid += 1\n",
    "\n",
    "    grouped = {}\n",
    "    article_id = 1\n",
    "\n",
    "    for category, keywords in CATEGORIES.items():\n",
    "        grouped.setdefault(category, {})\n",
    "        run_id = run_id_by_category[category]\n",
    "\n",
    "        for i, kw in enumerate(keywords, start=1):\n",
    "            print(f\" 진행 중: [{category}] ({i}/{len(keywords)}) {kw}\" + \" \" * 20, end=\"\\r\")\n",
    "\n",
    "            item = None\n",
    "            try:\n",
    "                item = fetch_one_naver_news(kw)\n",
    "            except Exception:\n",
    "                item = None\n",
    "\n",
    "            row = {\n",
    "                \"article_id\": article_id,\n",
    "                \"run_id\": run_id,\n",
    "                \"keyword_id\": keyword_id_by_text[kw],\n",
    "                \"title\": (item.get(\"title\") if item else None),\n",
    "                \"url\": (item.get(\"url\") if item else None),\n",
    "                \"publisher\": (item.get(\"publisher\") if item else None),\n",
    "                \"published_at\": (item.get(\"published_at\") if item else None),\n",
    "                \"image_url\": (item.get(\"image_url\") if item else None),\n",
    "                \"collected_at\": collected_at\n",
    "            }\n",
    "\n",
    "            # ✅ row는 DB키만\n",
    "            row = {k: row[k] for k in [\n",
    "                \"article_id\", \"run_id\", \"keyword_id\",\n",
    "                \"title\", \"url\", \"publisher\", \"published_at\",\n",
    "                \"image_url\", \"collected_at\"\n",
    "            ]}\n",
    "\n",
    "            grouped[category].setdefault(kw, []).append(row)\n",
    "\n",
    "            article_id += 1\n",
    "            time.sleep(0.5)\n",
    "\n",
    "    OUT_FILE = \"naver_news_grouped_by_category_keyword.json\"\n",
    "    with open(OUT_FILE, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(grouped, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "    print(f\"\\n✅ JSON 저장 완료: {OUT_FILE}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyexcel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
