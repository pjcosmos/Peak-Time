import os
import json
import time
import pandas as pd
from datetime import datetime, timezone, timedelta
from dotenv import load_dotenv
from googleapiclient.discovery import build
from googleapiclient.errors import HttpError

# 1. í™˜ê²½ ì„¤ì • ë° API ë¡œë“œ
load_dotenv()
API_KEY = os.getenv('YOUTUBE_API_KEY')
KST = timezone(timedelta(hours=9))

def format_kst_time(iso_date_str=None):
    """ìœ íŠœë¸Œì˜ UTC ì‹œê°„ì„ ë‰´ìŠ¤ ë°ì´í„°ì™€ ë™ì¼í•œ YYYY-MM-DD HH:MM (KST) í˜•ì‹ìœ¼ë¡œ ë³€í™˜"""
    if not iso_date_str:
        dt = datetime.now(KST)
    else:
        try:
            # ìœ íŠœë¸Œ APIëŠ” ISO8601(Z) í˜•ì‹ì„ ì£¼ë¯€ë¡œ ë³€í™˜ í•„ìš”
            dt = datetime.fromisoformat(iso_date_str.replace("Z", "+00:00"))
            dt = dt.astimezone(KST)
        except:
            return iso_date_str
    return dt.strftime("%Y-%m-%d %H:%M")

def get_youtube_data(keyword, keyword_id, run_id):
    """íŠ¹ì • í‚¤ì›Œë“œì— ëŒ€í•´ ìœ íŠœë¸Œ ìƒì„¸ ë°ì´í„°ë¥¼ ìˆ˜ì§‘"""
    if not API_KEY:
        print("ğŸš¨ API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        return None
    
    youtube = build('youtube', 'v3', developerKey=API_KEY)

    try:
        # 1ë‹¨ê³„: ê²€ìƒ‰ì„ í†µí•´ ì˜ìƒ ID 3ê°œ ì¶”ì¶œ (ìˆœìœ„ ë³´ì¡´)
        search_res = youtube.search().list(
            q=keyword,
            part='id',
            maxResults=3,
            type='video',
            regionCode='KR',
            order='relevance' # ê´€ë ¨ì„± ìˆœ (ìœ í–‰ ë°˜ì˜)
        ).execute()

        video_ids = [item['id']['videoId'] for item in search_res.get('items', []) if 'videoId' in item['id']]
        
        if not video_ids:
            return []

        # 2ë‹¨ê³„: ì˜ìƒ IDë“¤ë¡œ ìƒì„¸ ì§€í‘œ(ì¡°íšŒìˆ˜ ë“±) ìˆ˜ì§‘
        video_res = youtube.videos().list(
            part='statistics,snippet',
            id=','.join(video_ids)
        ).execute()

        collected_at = format_kst_time()
        youtube_rows = []
        
        for video in video_res.get('items', []):
            stats = video.get('statistics', {})
            snippet = video.get('snippet', {})
            
            youtube_rows.append({
                "run_id": run_id,               
                "keyword_id": keyword_id,       
                "youtube_id": video['id'],
                "title": snippet.get('title'),
                "channel_title": snippet.get('channelTitle'),
                "published_at": format_kst_time(snippet.get('publishedAt')),
                "view_count": int(stats.get('viewCount', 0)),
                "like_count": int(stats.get('likeCount', 0)),
                "comment_count": int(stats.get('commentCount', 0)),
                "thumbnail_url": snippet.get('thumbnails', {}).get('high', {}).get('url'),
                "collected_at": collected_at
            })
        return youtube_rows

    except HttpError as e:
        if e.resp.status == 403:
            return "QUOTA_EXCEEDED"
        print(f"âŒ API ì—ëŸ¬ ë°œìƒ: {e}")
        return []

# =========================
# ë©”ì¸ ì‹¤í–‰ë¶€
# =========================
if __name__ == "__main__":
    # ì¡°ì›ì´ ìƒì„±í•œ ë‰´ìŠ¤ ë°ì´í„° íŒŒì¼ ë¡œë“œ
    news_file_path = r'C:\git_down\Peak-Time\news\daum_news_grouped_by_category_keyword.json'
    
    if not os.path.exists(news_file_path):
        print(f"ğŸš¨ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {news_file_path}")
        exit()

    with open(news_file_path, 'r', encoding='utf-8') as f:
        news_data = json.load(f)

    final_db_data = []
    is_halted = False

    # ë‰´ìŠ¤ ë°ì´í„°ì˜ [ì¹´í…Œê³ ë¦¬] -> [í‚¤ì›Œë“œ] êµ¬ì¡°ë¥¼ ê·¸ëŒ€ë¡œ ë”°ë¼ê° (ìˆœì„œ ë³´ì¥)
    for category, keywords_dict in news_data.items():
        if is_halted: break
        print(f"\nğŸ“‚ ì¹´í…Œê³ ë¦¬ ìˆ˜ì§‘ ì¤‘: {category}")

        for keyword, articles in keywords_dict.items():
            if not articles: continue
            
            # (ê° í‚¤ì›Œë“œì˜ ì²« ë²ˆì§¸ ê¸°ì‚¬ ê°ì²´ì—ì„œ IDë¥¼ ì°¸ì¡°)
            target_run_id = articles[0]['run_id']
            target_keyword_id = articles[0]['keyword_id']
            
            print(f"  â””â”€ í‚¤ì›Œë“œ: '{keyword}' (ID: {target_keyword_id}) ìˆ˜ì§‘ ì‹œì‘...", end="", flush=True)
            
            # ìœ íŠœë¸Œ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
            results = get_youtube_data(keyword, target_keyword_id, target_run_id)
            
            if results == "QUOTA_EXCEEDED":
                print("\nğŸ›‘ ìœ íŠœë¸Œ API í• ë‹¹ëŸ‰ì´ ì´ˆê³¼ë˜ì—ˆìŠµë‹ˆë‹¤. ìˆ˜ì§‘ì„ ì¤‘ë‹¨í•©ë‹ˆë‹¤.")
                is_halted = True
                break
            
            if results:
                final_db_data.extend(results)
                print(f" ì™„ë£Œ ({len(results)}ê°œ ì˜ìƒ)")
            else:
                print(" ë°ì´í„° ì—†ìŒ")
            
            time.sleep(0.5) # API ë§¤ë„ˆ íƒ€ì„

    # ìµœì¢… ì €ì¥ (JSON ë° CSV)
    if final_db_data:
        # 1. DB ì ì¬ìš© ì „ì²´ ë°ì´í„° ì €ì¥
        with open("youtube_data_integrated.json", "w", encoding="utf-8") as f:
            json.dump(final_db_data, f, ensure_ascii=False, indent=4)
        
        # 2. ë¶„ì„ ë° í™•ì¸ìš© CSV ì €ì¥
        df = pd.DataFrame(final_db_data)
        df.to_csv("youtube_data_integrated.csv", index=False, encoding='utf-8-sig')

        # 3. ë¶„ì„íŒ€ì„ ìœ„í•œ í‚¤ì›Œë“œë³„ ìš”ì•½ íŒŒì¼ (í•©ê³„ ì§€í‘œ)
        summary = df.groupby(['run_id', 'keyword_id']).agg({
            'view_count': 'sum',
            'like_count': 'sum',
            'comment_count': 'sum'
        }).reset_index()
        summary.to_csv("youtube_keyword_summary.csv", index=False, encoding='utf-8-sig')

        print("\n" + "="*50)
        print(f"âœ¨ ìˆ˜ì§‘ ë° ë™ê¸°í™” ì™„ë£Œ!")
        print(f"ğŸ“Š ì´ ìˆ˜ì§‘ëœ ì˜ìƒ ìˆ˜: {len(final_db_data)}ê°œ")
        print(f"ğŸ“ ì €ì¥ íŒŒì¼: youtube_data_integrated.json / csv")
        print("="*50)