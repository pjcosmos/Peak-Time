import os
import json
import pandas as pd
from datetime import datetime, timezone, timedelta
from dotenv import load_dotenv
from googleapiclient.discovery import build
from googleapiclient.errors import HttpError

# 1. í™˜ê²½ ì„¤ì • ë° ì‹œê°„ëŒ€ ì •ì˜
load_dotenv()
API_KEY = os.getenv('YOUTUBE_API_KEY')
KST = timezone(timedelta(hours=9))

def format_kst_time(iso_date_str=None):
    """ìœ íŠœë¸Œì˜ UTC ì‹œê°„ì„ KST(YYYY-MM-DD HH:MM) í¬ë§·ìœ¼ë¡œ ë³€í™˜"""
    if not iso_date_str:
        dt = datetime.now(KST)
    else:
        # ìœ íŠœë¸Œ ë‚ ì§œ í¬ë§·(2026-02-25T05:30:00Z) ì²˜ë¦¬
        dt = datetime.fromisoformat(iso_date_str.replace("Z", "+00:00"))
        dt = dt.astimezone(KST)
    return dt.strftime("%Y-%m-%d %H:%M")

def get_youtube_data_for_db(keyword, keyword_id, run_id):
    """
    íŠ¹ì • í‚¤ì›Œë“œì— ëŒ€í•´ ìœ íŠœë¸Œ APIë¥¼ í˜¸ì¶œí•˜ê³ 
    DB í…Œì´ë¸”(youtube_video) êµ¬ì¡°ì— ë§ëŠ” ë¦¬ìŠ¤íŠ¸ë¥¼ ë°˜í™˜
    """
    if not API_KEY:
        print("ğŸš¨ API_KEYê°€ ì—†ìŠµë‹ˆë‹¤!")
        return None

    youtube = build('youtube', 'v3', developerKey=API_KEY)

    try:
        # [A] ê²€ìƒ‰ ìˆ˜í–‰ (maxResults=3)
        search_res = youtube.search().list(
            q=keyword, 
            part='id', 
            maxResults=3, 
            type='video', 
            regionCode='KR'
        ).execute()

        video_ids = [item['id']['videoId'] for item in search_res.get('items', []) if 'videoId' in item['id']]
        
        if not video_ids:
            return []

        # [B] ì˜ìƒ ìƒì„¸ ì •ë³´ ë° í†µê³„ ìˆ˜ì§‘
        video_res = youtube.videos().list(
            part='statistics,snippet', 
            id=','.join(video_ids)
        ).execute()

        collected_at = format_kst_time() # ìˆ˜ì§‘ ì‹œì  (KST)
        
        youtube_rows = []
        for video in video_res.get('items', []):
            stats = video.get('statistics', {})
            snippet = video.get('snippet', {})
            
            # DB youtube_video í…Œì´ë¸” ì»¬ëŸ¼ 1:1 ë§¤ì¹­
            row = {
                "run_id": run_id,                         # FK
                "keyword_id": keyword_id,                 # FK
                "youtube_id": video['id'],                # videoId
                "title": snippet.get('title'),
                "channel_title": snippet.get('channelTitle'),
                "published_at": format_kst_time(snippet.get('publishedAt')), # ë°œí–‰ì¼ KST ë³€í™˜
                "view_count": int(stats.get('viewCount', 0)),
                "like_count": int(stats.get('likeCount', 0)),
                "comment_count": int(stats.get('commentCount', 0)),
                "thumbnail_url": snippet.get('thumbnails', {}).get('high', {}).get('url'),
                "collected_at": collected_at
            }
            youtube_rows.append(row)
        
        return youtube_rows

    except HttpError as e:
        if e.resp.status == 403:
            print(f"ğŸ›‘ í• ë‹¹ëŸ‰ ì´ˆê³¼! ìˆ˜ì§‘ì„ ì¤‘ë‹¨í•©ë‹ˆë‹¤. (í‚¤ì›Œë“œ: {keyword})")
            return "QUOTA_EXCEEDED"
        return []

# 2. ë©”ì¸ ì‹¤í–‰ë¶€
if __name__ == "__main__":
    # [ë°ì´í„° ë¡œë“œ] ë¶„ì„íŒ€ì˜ í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸
    with open('../naver_data/collection_summary.json', 'r', encoding='utf-8') as f:
        config_data = json.load(f)

    # [ì„ì‹œ ID ë§¤í•‘] ì‹¤ì œ DBì™€ ì—°ê²° ì „, ë‰´ìŠ¤ ì¡°ì›ê³¼ ë™ì¼í•œ ë°©ì‹ìœ¼ë¡œ ID ìƒì„±
    # ì‹¤ì œ ìš´ì˜ ì‹œì—ëŠ” DBì—ì„œ í‚¤ì›Œë“œ/ëŸ° IDë¥¼ ì¡°íšŒí•´ì™€ì•¼ í•©ë‹ˆë‹¤.
    keyword_id_map = {}
    k_id_counter = 1
    run_id_map = {}
    r_id_counter = 1

    final_db_data = []
    is_halted = False

    for category, info in config_data.items():
        if is_halted: break
        
        # ì¹´í…Œê³ ë¦¬ë³„ run_id í• ë‹¹
        if category not in run_id_map:
            run_id_map[category] = r_id_counter
            r_id_counter += 1
        
        print(f"\nğŸ“‚ ì¹´í…Œê³ ë¦¬ ìˆ˜ì§‘: {category} (Run ID: {run_id_map[category]})")

        for kw in info['keywords']:
            # í‚¤ì›Œë“œë³„ keyword_id í• ë‹¹
            if kw not in keyword_id_map:
                keyword_id_map[kw] = k_id_counter
                k_id_counter += 1
            
            print(f"  > '{kw}' ìˆ˜ì§‘ ì¤‘... (ID: {keyword_id_map[kw]})")
            
            results = get_youtube_data_for_db(kw, keyword_id_map[kw], run_id_map[category])
            
            if results == "QUOTA_EXCEEDED":
                is_halted = True
                break
            
            if results:
                final_db_data.extend(results)

    # 3. ê²°ê³¼ ì €ì¥ (JSON & CSV)
    # DBì— ë°”ë¡œ Insertí•˜ê¸° ê°€ì¥ ì¢‹ì€ í˜•íƒœëŠ” JSON ë¦¬ìŠ¤íŠ¸ì…ë‹ˆë‹¤.
    if final_db_data:
        # ì „ì²´ ìƒì„¸ ë°ì´í„° (DB youtube_video í…Œì´ë¸”ìš©)
        df = pd.DataFrame(final_db_data)
        df.to_csv("youtube_data.csv", index=False, encoding='utf-8-sig')
        
        with open("youtube_data.json", "w", encoding="utf-8") as f:
            json.dump(final_db_data, f, ensure_ascii=False, indent=4)

        # ë¶„ì„íŒ€ ë³´ê³ ìš© ìš”ì•½ë³¸ (í‚¤ì›Œë“œë³„ í†µê³„ í†µí•©)
        df_summary = df.groupby('keyword_id').agg({
            'run_id': 'first',
            'view_count': 'sum',
            'like_count': 'sum',
            'comment_count': 'sum'
        }).reset_index()
        df_summary.to_csv("youtube_final_summary.csv", index=False, encoding='utf-8-sig')

        print(f"\nâœ… ìˆ˜ì§‘ ì™„ë£Œ! ì´ {len(final_db_data)}ê°œ ì˜ìƒ ë°ì´í„° ì €ì¥ë¨.")
        print("- DBìš©: youtube_db_ready.json / .csv")
        print("- ìš”ì•½ìš©: youtube_final_summary.csv")