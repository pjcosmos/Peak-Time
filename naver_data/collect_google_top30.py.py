import os
import json
import time
import urllib.request
from datetime import datetime
from dotenv import load_dotenv
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By

# 1. í™˜ê²½ ì„¤ì • ë° API í‚¤ ë¡œë“œ
load_dotenv()
NAVER_ID = os.getenv("NAVER_CLIENT_ID")
NAVER_SECRET = os.getenv("NAVER_CLIENT_SECRET")

def get_integrated_analysis_final_ultra():
    # ë¶„ì„ ëŒ€ìƒ ì¹´í…Œê³ ë¦¬ (ê¸ˆìœµ, ìŠ¤í¬ì¸ , ì—”í„°, ê¸°í›„)
    target_urls = {
        'finance': 'https://trends.google.co.kr/trending?geo=KR&hl=ko&hours=168&category=3',
        'sports': 'https://trends.google.co.kr/trending?geo=KR&hl=ko&hours=168&category=17',
        'entertainment': 'https://trends.google.co.kr/trending?geo=KR&hl=ko&hours=168&category=4',
        'climate': 'https://trends.google.co.kr/trending?geo=KR&hl=ko&hours=168&category=20'
    }
    
    options = Options()
    options.add_argument("--window-size=1920,1080")
    # ë´‡ íƒì§€ ë°©ì§€ë¥¼ ìœ„í•œ ìœ ì € ì—ì´ì „íŠ¸ ì„¤ì •
    options.add_argument("user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36")
    options.add_experimental_option("excludeSwitches", ["enable-automation"])
    
    driver = webdriver.Chrome(options=options)
    summary_report = {}

    # [í•µì‹¬] í´ë˜ìŠ¤ëª… ëŒ€ì‹  íƒœê·¸ êµ¬ì¡°(tr > td)ë¡œ ì ‘ê·¼í•˜ëŠ” ë²”ìš© ìŠ¤í¬ë¦½íŠ¸
    # innerTextì™€ textContentë¥¼ ëª¨ë‘ í™œìš©í•´ ë¡œë”© ì§€ì—° ë°ì´í„°ë¥¼ ê°•ì œ ì¶”ì¶œí•©ë‹ˆë‹¤.
    precision_scan_script = r"""
    let snapshot = [];
    let rows = document.querySelectorAll('tr');
    
    rows.forEach(row => {
        let cells = row.querySelectorAll('td');
        // êµ¬ê¸€ íŠ¸ë Œë“œ í‘œ êµ¬ì¡°ìƒ ì¹¸ì´ 3ê°œ ì´ìƒì¸ ê³³ì— ë°ì´í„°ê°€ ìˆìŠµë‹ˆë‹¤.
        if(cells.length >= 3) {
            // 2ë²ˆì§¸ ì¹¸(index 1) = í‚¤ì›Œë“œ, 3ë²ˆì§¸ ì¹¸(index 2) = ê²€ìƒ‰ëŸ‰
            let titleText = (cells[1].innerText || cells[1].textContent || "").split('\n')[0].trim();
            let volumeText = (cells[2].innerText || cells[2].textContent || "").trim();
            
            if(titleText && titleText.length > 1) {
                snapshot.push({title: titleText, google_volume: volumeText});
            }
        }
    });
    return snapshot;
    """

    for label, url in target_urls.items():
        print(f"\nğŸ“¡ [{label}] ì •ë°€ êµ¬ì¡° ìŠ¤ìº” í”„ë¡œì„¸ìŠ¤ ê°€ë™...")
        driver.get(url)
        time.sleep(12) # í˜ì´ì§€ ë° ë°ì´í„° ë¡œë”© ëŒ€ê¸°

        collected_dict = {} # ì¤‘ë³µ ì œê±°ìš© ì €ì¥ì†Œ
        
        # ì´˜ì´˜í•˜ê²Œ ë‚´ë ¤ê°€ë©° í™”ë©´ì— ê±¸ë¦¬ëŠ” ëª¨ë“  ê²ƒì„ ë‚šì•„ì±•ë‹ˆë‹¤. (25ë‹¨ê³„ ìŠ¤ìº”)
        for step in range(25): 
            current_snapshot = driver.execute_script(precision_scan_script)
            
            new_finds = 0
            for item in current_snapshot:
                title = item['title']
                # ì‹œê°„ ì •ë³´ ë° ìˆ«ìë§Œ ìˆëŠ” ë…¸ì´ì¦ˆ ì œê±°
                if not title.isdigit() and "ì‹œê°„ ì „" not in title and "ë¶„ ì „" not in title:
                    if title not in collected_dict:
                        collected_dict[title] = item['google_volume']
                        new_finds += 1
            
            # ëª©í‘œì¹˜(35ê°œ) í™•ë³´ ì‹œ ì¡°ê¸° ì¢…ë£Œ
            if len(collected_dict) >= 35:
                print(f"   > ëª©í‘œ ìˆ˜ëŸ‰ ì¶©ë¶„ í™•ë³´ ({len(collected_dict)}ê°œ)")
                break
                
            # ìŠ¤í¬ë¡¤ ë‹¨ìœ„ë¥¼ 300pxë¡œ ì¢í˜€ì„œ ì•„ì£¼ ê¼¼ê¼¼í•˜ê²Œ í›‘ìŠµë‹ˆë‹¤.
            driver.execute_script("window.scrollBy(0, 300);")
            time.sleep(2.5) # êµ¬ê¸€ ì„œë²„ê°€ ë°ì´í„°ë¥¼ ì±„ì›Œë„£ì„ ì¶©ë¶„í•œ ì‹œê°„ ë¶€ì—¬
            
            if new_finds > 0:
                print(f"   > {step+1}ë‹¨ê³„: ëˆ„ê³„ {len(collected_dict)}ê°œ (ìƒˆ í‚¤ì›Œë“œ {new_finds}ê°œ í¬ì°©)")

        # ìµœì¢… TOP 30 ìŠ¬ë¼ì´ì‹±
        final_list = [{"title": t, "google_volume": v} for t, v in collected_dict.items()][:30]
        
        if not final_list:
            print(f"âš ï¸ {label}ì—ì„œ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. (í˜ì´ì§€ ë¡œë”© í™•ì¸ í•„ìš”)")
            continue

        print(f"âœ… {label}: ìµœì¢… {len(final_list)}ê°œ í‚¤ì›Œë“œ í™•ì •")

        # 2. ë„¤ì´ë²„ API ì—°ë™ ë° ê²°ê³¼ ë³‘í•©
        titles = [it['title'] for it in final_list]
        naver_raw = fetch_naver_data(titles)

        final_data_list = []
        for item in final_list:
            n_data = next((res['data'] for res in naver_raw if res['title'] == item['title']), [])
            ratio_sum = round(sum(day['ratio'] for day in n_data), 2)
            
            final_data_list.append({
                "rank_title": item['title'],
                "google_volume": item['google_volume'],
                "naver_trend_sum": ratio_sum,
                "naver_daily_ratio": n_data
            })

        # 3. ê°œë³„ ê²°ê³¼ ì €ì¥
        save_path = f'trend_report_{label}.json'
        with open(save_path, 'w', encoding='utf-8') as f:
            json.dump({
                "category": label,
                "base_date": datetime.now().strftime('%Y-%m-%d'),
                "results": final_data_list
            }, f, ensure_ascii=False, indent=4)
        
        summary_report[label] = {
            "total_count": len(final_data_list),
            "keywords": [x['rank_title'] for x in final_data_list]
        }

    # 4. ì „ì²´ ìš”ì•½ ë¦¬í¬íŠ¸ ì €ì¥
    with open('collection_summary.json', 'w', encoding='utf-8') as f:
        json.dump(summary_report, f, ensure_ascii=False, indent=4)
    
    print("\nâœ¨ ë¶„ì„ ì™„ë£Œ! ê²°ê³¼ëŠ” 'trend_report_*.json' ë° 'collection_summary.json' íŒŒì¼ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    driver.quit()

def fetch_naver_data(keywords):
    if not keywords: return []
    url = "https://openapi.naver.com/v1/datalab/search"
    results = []
    for i in range(0, len(keywords), 5):
        chunk = keywords[i:i+5]
        body = {
            "startDate": "2026-02-18",
            "endDate": datetime.now().strftime('%Y-%m-%d'),
            "timeUnit": "date",
            "keywordGroups": [{"groupName": k, "keywords": [k]} for k in chunk]
        }
        req = urllib.request.Request(url)
        req.add_header("X-Naver-Client-Id", NAVER_ID)
        req.add_header("X-Naver-Client-Secret", NAVER_SECRET)
        req.add_header("Content-Type", "application/json")
        try:
            res = urllib.request.urlopen(req, data=json.dumps(body).encode("utf-8"))
            results.extend(json.loads(res.read())['results'])
            time.sleep(0.5)
        except:
            pass
    return results

if __name__ == "__main__":
    get_integrated_analysis_final_ultra()